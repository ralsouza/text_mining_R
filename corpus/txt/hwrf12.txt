Hybrid weighted random forests for
classifying very high-dimensional data
Baoxun Xu1 , Joshua Zhexue Huang2 , Graham Williams2 and
Yunming Ye1
1

Department of Computer Science, Harbin Institute of Technology Shenzhen Graduate
School, Shenzhen 518055, China
2
Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen
518055, China
Email: amusing002@gmail.com
Random forests are a popular classiﬁcation method based on an ensemble of a
single type of decision trees from subspaces of data. In the literature, there
are many diﬀerent types of decision tree algorithms, including C4.5, CART, and
CHAID. Each type of decision tree algorithm may capture diﬀerent information
and structure. This paper proposes a hybrid weighted random forest algorithm,
simultaneously using a feature weighting method and a hybrid forest method to
classify very high dimensional data. The hybrid weighted random forest algorithm
can eﬀectively reduce subspace size and improve classiﬁcation performance
without increasing the error bound. We conduct a series of experiments on eight
high dimensional datasets to compare our method with traditional random forest
methods and other classiﬁcation methods. The results show that our method
consistently outperforms these traditional methods.
Keywords: Random Forests; Hybrid Weighted Random Forest; Classification; Decision tree;

1.

INTRODUCTION

Random forests [1, 2] are a popular classiﬁcation
method which builds an ensemble of a single type
of decision trees from diﬀerent random subspaces of
data. The decision trees are often either built using
C4.5 [3] or CART [4], but only one type within
a single random forest. In recent years, random
forests have attracted increasing attention due to
(1) its competitive performance compared with other
classiﬁcation methods, especially for high-dimensional
data, (2) algorithmic intuitiveness and simplicity, and
(3) its most important capability - “ensemble” using
bagging [5] and stochastic discrimination [2].
Several methods have been proposed to grow random
forests from subspaces of data [1, 2, 6, 7, 8, 9, 10]. In
these methods, the most popular forest construction
procedure was proposed by Breiman [1] to ﬁrst use
bagging to generate training data subsets for building
individual trees.
A subspace of features is then
randomly selected at each node to grow branches of
a decision tree. The trees are then combined as an
ensemble into a forest. As an ensemble learner, the
performance of a random forest is highly dependent
on two factors: the performance of each tree and the
diversity of the trees in the forests [11]. Breiman
formulated the overall performance of a set of trees as
the average strength and proved that the generalization

error of a random forest is bounded by the ratio of the
average correlation between trees divided by the square
of the average strength of the trees.
For very high dimensional data, such as text data,
there are usually a large portion of features that are
uninformative to the classes. During this forest building
process, informative features would have the large
chance to be missed, if we randomly select a small
subspace (Breiman suggested selecting ⌊log2 (M ) + 1⌋
features in a subspace, where M is the number of
independent features in the data) from high dimensional
data [12]. As a result, weak trees are created from these
subspaces, the average strength of those trees is reduced
and the error bound of the random forest is enlarged.
Therefore, when a large proportion of such “weak”
trees are generated in a random forest, the forest has a
large likelihood to make a wrong decision which mainly
results from those “weak” trees’ classiﬁcation power.
To address this problem, we aim to optimize decision
trees of a random forest by two strategies. One
straightforward strategy is to enhance the classiﬁcation
performance of individual trees by a feature weighting
method for subspace sampling [12, 13, 14]. In this
method, feature weights are computed with respect
to the correlations of features to the class feature
and regarded as the probabilities of the feature to
be selected in subspaces. This method obviously
increases the classiﬁcation performance of individual

The Computer Journal, Vol. ??,

No. ??,

????

2

Baoxun Xu, Joshua Zhexue Huang, Graham Williams, Yunming Ye

trees because the subspaces will be biased to contain
more informative features. However, the chance of more
correlated trees is also increased since the features with
large weights are likely to be repeatedly selected.
The second strategy is more straightforward: use
several diﬀerent types of decision trees for each training
data subset, to increase the diversity of the trees,
and then select the optimal tree as the individual
tree classiﬁer in the random forest model. The work
presented here extends the algorithm developed in [15].
Speciﬁcally, we build three diﬀerent types of tree
classiﬁers (C4.5, CART, and CHAID [16, 17]) for each
training data subset. We then evaluate the performance
of the three classiﬁers and select the best tree. In
this way, we build a hybrid random forest which may
include diﬀerent types of decision trees in the ensemble.
The added diversity of the decision trees can eﬀectively
improve the accuracy of each tree in the forest, and
hence the classiﬁcation performance of the ensemble.
However, when we use this method to build the best
random forest model for classifying high dimensional
data, we can not be sure of what subspace size is best.
In this paper, we propose a hybrid weighted random
forest algorithm by simultaneously using a new feature
weighting method together with the hybrid random
forest method to classify high dimensional data. In
this new random forest algorithm, we calculate feature
weights and use weighted sampling to randomly select
features for subspaces at each node in building diﬀerent
types of trees classiﬁers (C4.5, CART, and CHAID) for
each training data subset, and select the best tree as
the individual tree in the ﬁnal ensemble model.
Experiments were performed on 8 high dimensional
text datasets with dimensions ranging from 2000 to
13195. We compared the performance of eight random
forest methods and well-known classiﬁcation methods:
C4.5 random forest, CART random forest, CHAID
random forest, hybrid random forest, C4.5 weighted
random forest, CART weighted random forest, CHAID
weighted random forest, hybrid weighted random
forest, support vector machines [18], naive Bayes [19],
and k-nearest neighbors [20].
The experimental
results show that our hybrid weighted random forest
achieves improved classiﬁcation performance over the
ten competitive methods.
The remainder of this paper is organized as follows.
In Section 2, we introduce a framework for building
a hybrid weighted random forest, and describe a new
random forest algorithm. Section 3 summarizes four
measures to evaluate random forest models. We present
experimental results on 8 high dimensional text datasets
in Section 4. Section 5 contains our conclusions.

TABLE 1. Contingency table of input feature A and class
feature Y
Y = y1 . . .
Y = yj . . .
Y = yq Total
A = a1
σ11
...
σ1j
...
σ1q
σ1·
..
..
..
..
..
..
.
.
...
.
.
.
.
A = ai
σi1
...
σij
...
σiq
σi·
..
..
..
..
..
..
...
.
.
.
.
.
.
A = ap
σp1
...
σpj
...
σpq
σp·
Total
σ·1
...
σ·j
...
σ·q
σ

general framework for building hybrid random forests.
By integrating these two methods, we propose a novel
hybrid weighted random forest algorithm.
2.1.

Let Y be the class (or target) feature with q distinct
class labels yj for j = 1, · · · , q. For the purposes of
our discussion we consider a single categorical feature
A in dataset D with p distinct category values. We
denote the distinct values by ai for i = 1, · · · , p.
Numeric features can be discretized into p intervals with
a supervised discretization method.
Assume D has val objects. The size of the subset of
D satisfying the condition that A = ai and Y = yj is
denoted by σij . Considering all combinations of the
categorical values of A and the labels of Y , we can
obtain a contingency table [21] of A against Y as shown
in Table 1. The far right column contains the marginal
totals for feature A:

HYBRID
FORESTS

WEIGHTED

RANDOM

In this section, we ﬁrst introduce a feature weighting
method for subspace sampling. Then we present a

q
∑

σi. =

σij

for i = 1, · · · , p

(1)

j=1

and the bottom row is the marginal totals for class
feature Y :
σ.j =

p
∑

σij

for j = 1, · · · , q

(2)

i=1

The grand total (the total number of samples) is in
the bottom right corner:
σ=

q ∑
p
∑

σij

(3)

j=1 i=1

Given a training dataset D and feature A we ﬁrst
compute the contingency table. The feature weights are
then computed using the two methods to be discussed
in the following subsection.
2.2.

2.

Notation

Feature Weighting Method

In this subsection, we give the details of the feature
weighting method for subspace sampling in random
forests. Consider an M-dimensional feature space
{A1 , A2 , . . . , AM }. We present how to compute the

The Computer Journal, Vol. ??,

No. ??,

????

Hybrid weighted random forests for classifying very high-dimensional data
weights {w1 , w2 , . . . , wM } for every feature in the space.
These weights are then used in the improved algorithm
to grow each decision tree in the random forest.
2.2.1. Feature Weight Computation
The weight of feature A represents the correlation
between the values of feature A and the values of the
class feature Y . A larger weight will indicate that the
class labels of objects in the training dataset are more
correlated with the values of feature A, indicating that
A is more informative to the class of objects. Thus it
is suggested that A has a stronger power in predicting
the classes of new objects.
In the following, we propose to use the chi-square
statistic to compute feature weights because this
method can quantify the correspondence between two
categorical variables.
Given the contingency table of an input feature A and
the class feature Y of dataset D, the chi-square statistic
of the two features is computed as:
corr(A, Y ) =

q
p ∑
∑
(σij − tij )2
tij
i=1 j=1

(4)

where σij is the observed frequency from the
contingency table and tij is the expected frequency
computed as
σi· × σ·j
tij =
σ

(5)

The larger the measure corr(A, Y ), the more
informative the feature A is in predicting class Y .
2.2.2. Normalized Feature Weight
In practice, feature weights are normalized for feature
subspace sampling. We use corr(A, Y ) to measure the
informativeness of these features and consider them
as feature weights. However, to treat the weights as
probabilities of features, we normalize the measures to
ensure the sum of the normalized feature weights is
equal to 1. Let corr(Ai , Y ) (1 ≤ i ≤ M ) be the set
of M feature measures. We compute the normalized
weights as
√
corr(Ai , Y )
wi = ∑N √
(6)
i=1 corr(Ai , Y )
Here, we use the square root to smooth the values of
the measures. wi can be considered as the probability
that feature Ai is randomly sampled in a subspace. The
more informative a feature is, the larger the weight and
the higher the probability of the feature being selected.

Diversity is commonly obtained by using bagging and
random subspace sampling. We introduce a further
element of diversity by using diﬀerent types of trees.
Considering an analogy with forestry, the diﬀerent data subsets from bagging represent the “soil structures.” Diﬀerent decision tree algorithms represent “different tree species”. Our approach has two key aspects:
one is to use three types of decision tree algorithms to
generate three diﬀerent tree classiﬁers for each training data subset; the other is to evaluate the accuracy
of each tree as the measure of tree importance. In this
paper, we use the out-of-bag accuracy to assess the importance of a tree.
Following Breiman [1], we use bagging to generate
a series of training data subsets from which we build
trees. For each tree, the data subset used to grow
the tree is called the “in-of-bag” (IOB) data and the
remaining data subset is called the “out-of-bag” (OOB)
data. Since OOB data is not used for building trees
we can use this data to objectively evaluate each tree’s
accuracy and importance. The OOB accuracy gives an
unbiased estimate of the true accuracy of a model.
Given n instances in a training dataset D and a tree
classiﬁer hk (IOBk ) built from the k’th training data
subset IOBk , we deﬁne the OOB accuracy of the tree
hk (IOBk ), for di ∈ D, as:
∑n
OOBAcck =

Framework for Building a Hybrid Random
Forest

As an ensemble learner, the performance of a random
forest is highly dependent on two factors: the diversity
among the trees and the accuracy of each tree [11].

i=1

I(hk (di ) = yi ; di ∈ OOBk )
∑n
i=1 I(di ∈ OOBk )

(7)

where I(.) is an indicator function. The larger the
OOBAcck , the better the classiﬁcation quality of a tree.
We use the out-of-bag data subset OOBi to calculate
the out-of-bag accuracies of the three types of trees
(C4.5, CART and CHAID) with evaluation values E1 ,
E2 and E3 respectively.
Fig. 1 illustrates the procedure for building a hybrid
random forest model. Firstly, a series of IOB/OOB
datasets are generated from the entire training dataset
by bagging. Then, three types of tree classiﬁers (C4.5,
CART and CHAID) are built using each IOB dataset.
The corresponding OOB dataset is used to calculate the
OOB accuracies of the three tree classiﬁers. Finally,
we select the tree with the highest OOB accuracy as
the ﬁnal tree classiﬁer, which is included in the hybrid
random forest.
Building a hybrid random forest model in this
way will increase the diversity among the trees.
The classiﬁcation performance of each individual tree
classiﬁer is also maximized.
2.4.

2.3.

3

Decision Tree Algorithms

The core of our approach is the diversity of decision
tree algorithms in our random forest. Diﬀerent decision
tree algorithms grow structurally diﬀerent trees from
the same training data. Selecting a good decision tree
algorithm to grow trees for a random forest is critical

The Computer Journal, Vol. ??,

No. ??,

????

4

Baoxun Xu, Joshua Zhexue Huang, Graham Williams, Yunming Ye
the diﬀerence lies in the way to split a node, such
as the split functions and binary branches or multibranches. In this work we use these diﬀerent decision
tree algorithms to build a hybrid random forest.

2.5.

FIGURE 1. The Hybrid Random Forests framework.

for the performance of the random forest. Few studies
have considered how diﬀerent decision tree algorithms
aﬀect a random forest. We do so in this paper.
The common decision tree algorithms are as follows:
Classification Trees 4.5 (C4.5) is a supervised
learning classiﬁcation algorithm used to construct
decision trees. Given a set of pre-classiﬁed objects, each
described by a vector of attribute values, we construct
a mapping from attribute values to classes. C4.5 uses
a divide-and-conquer approach to grow decision trees.
Beginning with the entire dataset, a tree is constructed
by considering each predictor variable for dividing the
dataset. The best predictor is chosen at each node
using a impurity or diversity measure. The goal is
to produce subsets of the data which are homogeneous
with respect to the target variable. C4.5 selects the test
that maximizes the information gain ratio (IGR) [3].
Classification and Regression Tree (CART) is
a recursive partitioning method that can be used for
both regression and classiﬁcation. The main diﬀerence
between C4.5 and CART is the test selection and
evaluation process.
Chi-squared Automatic Interaction Detector
(CHAID) method is based on the chi-square test of
association. A CHAID decision tree is constructed
by repeatedly splitting subsets of the space into two
or more nodes. To determine the best split at any
node, any allowable pair of categories of the predictor
variables is merged until there is no statistically
signiﬁcant diﬀerence within the pair with respect to the
target variable [16, 17].
From these decision tree algorithms, we can see that

Hybrid Weighted Random Forest Algorithm

In this subsection we present a hybrid weighted
random forest algorithm by simultaneously using the
feature weights and a hybrid method to classify high
dimensional data. The beneﬁts of our algorithm has
two aspects: Firstly, compared with hybrid forest
method [15], we can use a small subspace size to
create accurate random forest models.
Secondly,
compared with building a random forest using feature
weighting [14], we can use several diﬀerent types of
decision trees for each training data subset to increase
the diversities of trees. The added diversity of the
decision trees can eﬀectively improve the classiﬁcation
performance of the ensemble model. The detailed steps
are introduced in Algorithm 1.
Input parameters to Algorithm 1 include a training
dataset D, the set of features A, the class feature Y ,
the number of trees in the random forest K and the
size of subspaces m. The output is a random forest
model M . Lines 9–16 form the loop for building K
decision trees. In the loop, Line 10 samples the training
data D by sampling with replacement to generate an
in-of-bag data subset IOBi for building a decision tree.
Line 11–14 build three types of tree classiﬁers (C4.5,
CART, and CHAID). In this procedure, Line 12 calls
the function createT reej () to build a tree classiﬁer.
Line 13 calculates the out-of-bag accuracy of the tree
classiﬁer. After this procedure, Line 15 selects the tree
classiﬁer with the maximum out-of-bag accuracy. K
decision tree trees are thus generated to form a hybrid
weighted random forest model M .
Generically, function createT reej () ﬁrst creates a
new node. Then, it tests the stopping criteria to decide
whether to return to the upper node or to split this
node. If we choose to split this node, then the feature
weighting method is used to randomly select m features
as the subspace for node splitting. These features
are used as candidates to generate the best split to
partition the node. For each subset of the partition,
createT reej () is called again to create a new node under
the current node. If a leaf node is created, it returns to
the parent node. This recursive process continues until
a full tree is generated.

The Computer Journal, Vol. ??,

No. ??,

????

Hybrid weighted random forests for classifying very high-dimensional data
Algorithm 1 New Random Forest Algorithm
1: Input:
2: - D : the training dataset,
3: - A : the features space {A1 , A2 , ..., AM },
4: - Y : the class features space {y1 , y2 , ..., yq },
5: - K : the number of trees,
6: - m : the size of subspaces.
7: Output: A random forest M ;
8: Method:
9: for i = 1 to K do
10:
draw a bootstrap sample in-of-bag data subset
IOBi and out-of-bag data subset OOBi from
training dataset D;
11:
for j = 1 to 3 do
12:
hi,j (IOBi ) = createT reej ();
use out-of-bag data subset OOBi to calculate
13:
the out-of-bag accuracy OOBAcci, j of the tree
classiﬁer hi,j (IOBi ) by Equation(1);
14:
end for
15:
select hi (IOBi ) with the highest out-of-bag
accuracy OOBAcci as Optimal tree i;
16: end for
17: combine
the
K
tree
classiﬁers
h1 (IOB1 ), h2 (IOB2 ), ..., hK (IOBK ) into a random
forest M ;
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:
31:
32:

3.

Function createTree()
create a new node N ;
if stopping criteria is met then
return N as a leaf node;
else
for j = 1 to M do
compute
the
informativeness
measure
corr(Aj , Y ) by Equation (4);
end for
compute feature weights {w1 , w2 , ..., wM } by
Equation (6);
use the feature weighting method to randomly
select m features;
use these m features as candidates to generate
the best split for the node to be partitioned;
call createTree() for each split;
end if
return N ;
EVALUATION MEASURES

In this paper, we use ﬁve measures, i.e., strength,
correlation, error bound c/s2 , test accuracy, and F1
metric, to evaluate our random forest models. Strength
measures the collective performance of individual trees
in a random forest and the correlation measures the
diversity of the trees. The ratio of the correlation
over the square of the strength c/s2 indicates the
generalization error bound of the random forest model.
These three measures were introduced in [1]. The
accuracy measures the performance of a random forest
model on unseen test data. The F1 metric is a

5

commonly used measure of classiﬁcation performance.
3.1.

Strength and Correlation Measures

We follow Breiman’s method described in [1] to
calculate the strength, correlation and the ratio c/s2 .
Following Breiman’s notation, we denote strength as
s and correlation as ρ̄. Let hk (IOBk ) be the kth
tree classiﬁer grown from the kth training data IOBk
sampled from D with replacement.
Assume the
random forest model contains K trees. The out-of-bag
proportion of votes for di ∈ D on class j is
∑K
I(hk (di ) = j; di ∈
/ IOBk )
Q(di , j) = k=1∑K
(8)
/ IOBk )
k=1 I(di ∈
This is the number of trees in the random forest
which are trained without di and classify di into class
j, divided by the number of training datasets not
containing di .
The strength s is computed as:
1∑
(Q(di , yi ) − maxj̸=yi Q(di , j))
n i=1
n

s=

(9)

where n is the number of objects in D and yi indicates
the true class of di .
The correlation ρ̄ is computed as:
∑n
1
2
2
i=1 (Q(di , yi ) − maxj̸=yi Q(di , j)) − s
n
(10)
ρ̄ =
√
∑
K
1
2 2
(K
k=1 pk + p̄k + (pk − p̄k ) )
where

∑n
pk =

i=1

I(hk (di ) = yi ; di ∈
/ IOBk )
∑n
/ IOBk )
i=1 I(di ∈

(11)

and
∑n
p̄k =

i=1

I(hk (di ) = ĵ(di , Y ); di ∈
/ IOBk )
∑n
I(d
∈
/
IOB
)
i
k
i=1

(12)

where
ĵ(di , Y ) = argmaxj̸=yi Q(d, j)

(13)

is the class that obtains the maximal number of votes
among all classes but the true class.
3.2.

General Error Bound Measure c/s2

Given the strength and correlation, the out-of-bag
estimate of the c/s2 measure can be computed.
An important theoretical result in Breiman’s method
is the upper bound of the generalization error of the
random forest ensemble that is derived as
P E∗ ≤ ρ(1 − s2 )/s2

(14)

where ρ̄ is the mean value of correlations between all
pairs of individual classiﬁers and s is the strength of
the set of individual classiﬁers that is estimated as the

The Computer Journal, Vol. ??,

No. ??,

????

6

Baoxun Xu, Joshua Zhexue Huang, Graham Williams, Yunming Ye

average accuracy of individual classiﬁers on D with
out-of-bag evaluation. This inequality shows that the
generalization error of a random forest is aﬀected by
the strength of individual classiﬁers and their mutual
correlations. Therefore, Breiman deﬁned the c/s2 ratio
to measure a random forest as
c/s2 = ρ̄/s2

(15)

The smaller the ratio, the better the performance of
the random forest. As such, c/s2 gives guidance for
reducing the generalization error of random forests.
3.3.

Test Accuracy

The test accuracy measures the classiﬁcation performance of a random forest on the test data set. Let
Dt be a test data and Yt be the class labels. Given
di ∈ Dt , the number of votes for di on class j is
N (di , j) =

K
∑

I(hk (di ) = j)

(16)

TABLE 2.
Summary statistic of 8 high-dimensional
datasets
Name
Features
Instances
Classes % Minority
Fbis
2000
2463
17
1.54
Re0
2886
1504
13
0.73
Re1
3758
1657
25
0.6
Tr41
7454
878
10
1.03
Wap
8460
1560
20
0.32
Tr31
10,128
927
7
0.22
La2s
12,432
3075
6
8.07
La1s
13,195
3204
6
8.52

It emphasizes the performance of a classiﬁer on rare
categories. Deﬁne α and β as follows:

αi =

T Pi
T Pi
, βi =
(T Pi + F Pi )
(T Pi + F Ni )

(20)

F 1 for each category i and the macro-averaged F1
are computed as:

k=1

The test accuracy is calculated as
F 1i =
1∑
I(N (di , yi ) − maxj̸=yi N (di , j) > 0) (17)
n i=1

2αi βi
, M acroF 1 =
αi + βi

∑q
i=1

q

F 1i

(21)

n

Acc =

where n is the number of objects in Dt and yi indicates
the true class of di .
3.4.

F1 Metric

To evaluate the performance of classiﬁcation methods
in dealing with an unbalanced class distribution, we use
the F1 metric introduced by Yang and Liu [22]. This
measure is equal to the harmonic mean of recall (α)
and precision (β). The overall F1 score of the entire
classiﬁcation problem can be computed by a microaverage and a macro-average.
Micro-averaged F1 is computed globally over all
classes, and emphasizes the performance of a classiﬁer
on common classes. Deﬁne α and β as follows:
∑q

∑q
T Pi
i=1 T Pi
α = ∑q i=1
, β = ∑q
(18)
i=1 (T Pi + F Pi )
i=1 (T Pi + F Ni )
where q is the number of classes. T Pi (True Positives)
is the number of objects correctly predicted as class i,
F Pi (False Positives) is the number of objects that are
predicted to belong to class i but do not. The microaveraged F1 is computed as:
M icroF 1 =

2αβ
α+β

(19)

Macro-averaged F1 is ﬁrst computed locally over
each class, and then the average over all classes is taken.

The larger the MicroF1 and MacroF1 values are, the
higher the classiﬁcation performance of the classiﬁer.
4.

EXPERIMENTS

In this section, we present two experiments that
demonstrate the eﬀectiveness of the new random
forest algorithm for classifying high dimensional data.
High dimensional datasets with various sizes and
characteristics were used in the experiments. The
ﬁrst experiment is designed to show how our proposed
method can reduce the generalization error bound
c/s2 , and improve test accuracy when the size of
the selected subspace is not too large. The second
experiment is used to demonstrate the classiﬁcation
performance of our proposed method in comparison to
other classiﬁcation methods, i.e. SVM, NB and KNN.
4.1.

Datasets

In the experiments, we used eight real-world high
dimensional datasets. These datasets were selected
due to their diversities in the number of features, the
number of instances, and the number of classes. Their
dimensionalities vary from 2000 to 13,195. Instances
vary from 878 to 3204 and the minority class rate varies
from 0.22% to 8.52%. In each dataset, we randomly
select 70% of instances as the training dataset, and
the remaining data as the test dataset. Detailed
information of the eight datasets is listed in Table 2.
The Fbis, Re0, Re1, Tr41, Wap, Tr31, La2s
and La1s datasets are classical text classiﬁcation
benchmark datasets which were carefully selected and

The Computer Journal, Vol. ??,

No. ??,

????

Hybrid weighted random forests for classifying very high-dimensional data
preprocessed by Han and Karypis [23]. Dataset Fbis
was compiled from the Foreign Broadcast Information
Service TREC-5 [24]. The datasets Re0 and Re1 were
selected from the Reuters-21578 text categorization test
collection Distribution 1.0 [25]. The datasets Tr41 and
Tr31 were derived from TREC-5 [24], TREC-6 [24],
and TREC-7 [24]. Dataset Wap is from the WebACE
project (WAP) [26]. The datasets La2s and La1s were
selected from the Los Angeles Times for TREC-5 [24].
The classes of these datasets were generated from the
relevance judgment provided in these collections.
4.2.

Performance Comparisons between Random Forest Methods

The purpose of this experiment was to evaluate
the eﬀect of the hybrid weighted random forest
method (H W RF) on strength, correlation, c/s2 ,
and test accuracy.
The eight high dimensional
datasets were analyzed and results were compared
with seven other random forest methods, i.e., C4.5
random forest (C4.5 RF), CART random forest
(CART RF), CHAID random forest (CHAID RF),
hybrid random forest (H RF), C4.5 weighted random
forest (C4.5 W RF), CART weighted random forest
(CART W RF), CHAID weighted random forest
(CHAID W RF). For each dataset, we ran each
random forest algorithm against diﬀerent sizes of the
feature subspaces. Since the number of features in these
datasets was very large, we started with a subspace
of 10 features and increased the subspace by 5 more
features each time. For a given subspace size, we built
100 trees for each random forest model. In order to
obtain a stable result, we built 80 random forest models
for each subspace size, each dataset and each algorithm,
and computed the average values of the four measures
of strength, correlation, c/s2 , and test accuracy as the
ﬁnal results for comparison. The performance of the
eight random forest algorithms on the four measures
for each of the 8 datasets is shown in Figs. 2, 3, 4, and
5.
Fig. 2 plots the strength for the eight methods against
diﬀerent subspace sizes on each of the 8 datasets.
In the same subspace, the higher the strength, the
better the result. From the curves, we can see that
the new algorithm (H W RF) consistently performs
better than the seven other random forest algorithms.
The advantages are more obvious for small subspaces.
The new algorithm quickly achieved higher strength
as the subspace size increases.
The seven other
random forest algorithms require larger subspaces to
achieve a higher strength. These results indicate that
the hybrid weighted random forest algorithm enables
random forest models to achieve a higher strength
for small subspace sizes compared to the seven other
random forest algorithms.
Fig. 3 plots the curves for the correlations for the
eight random forest methods on the 8 datasets. For

7

small subspace sizes, H RF, C4.5 RF, CART RF,
and CHAID RF produce higher correlations between
the trees on all datasets. The correlation decreases
as the subspace size increases. For the random forest
models the lower the correlation between the trees
then the better the ﬁnal model.
With our new
random forest algorithm (H W RF) a low correlation
level was achieved with very small subspaces in all
8 datasets. We also note that as the subspace size
increased the correlation level increased as well. This is
understandable because as the subspace size increases,
the same informative features are more likely to be
selected repeatedly in the subspaces, increasing the
similarity of the decision trees. Therefore, the feature
weighting method for subspace selection works well for
small subspaces, at least from the point of view of the
correlation measure.
Fig. 4 shows the error bound indicator c/s2 for the
eight methods on the 8 datasets. From these ﬁgures
we can observe that as the subspace size increases, c/s2
consistently reduces. The behaviour indicates that a
subspace size larger than ⌊log2 (M )+1⌋ beneﬁts all eight
algorithms. However, the new algorithm (H W RF)
achieved a lower level of c/s2 at subspace size of
⌊log2 (M ) + 1⌋ than the seven other algorithms.
Fig. 5 plots the curves showing the accuracy of the
eight random forest models on the test datasets from
the 8 datasets. We can clearly see that the new random
forest algorithm (H W RF) outperforms the seven
other random forest algorithms in all eight data sets.
It can be seen that the new method is more stable
in classiﬁcation performance than other methods. In
all of these ﬁgures, it is observed that the highest test
accuracy is often obtained with the default subspace size
of ⌊log2 (M ) + 1⌋. This implies that in practice, large
size subspaces are not necessary to grow high-quality
trees for random forests.
4.3.

Performance Comparisons
Classification Methods

with

Other

We conducted a further experimental comparison
against three other widely used text classiﬁcation
methods: support vector machines (SVM), Naive
Bayes (NB), and k-nearest neighbor (KNN). The
support vector machine used a linear Kernel with a
regularization parameter of 0.03125, which was often
used in text categorization. For Naive Bayes, we
adopted the multi-variate Bernoulli event model that
is frequently used in text classiﬁcation [27]. For knearest neighbor (KNN), we set the number k of
neighbors to 13. In the experiments, we used WEKA’s
implementation for these three text classiﬁcation
methods [28]. We used a single subspace size of
features in all eight datasets to run the random forest
algorithms. For H RF, C4.5 RF, CART RF, and
CHAID RF, we used a subspace size of 90 features in
the ﬁrst 6 datasets (i.e., Fbis, Re0, Re1, Tr41, Wap, and

The Computer Journal, Vol. ??,

No. ??,

????

8

Baoxun Xu, Joshua Zhexue Huang, Graham Williams, Yunming Ye
Fbis

Re0

0.52

0.52

0.48

0.48

0.44

Strength

Strength

0.44

0.40

0.40

H_W_RF

0.36

C4.5_W_RF
CART_W_RF

0.32

H_W_RF
C4.5_W_RF

0.36

CART_W_RF

CHAID_W_RF

CHAID_W_RF

0.32

H_RF

0.28

H_RF

C4.5_RF

C4.5_RF

0.28

CART_RF

0.24

CART_RF

CHAID_RF

CHAID_RF

0.24
10

20

30

40

50

60

70

80

90

100

10

20

30

50

60

70

80

90

100

Number of features

Number of features

Re1

0.60

40

Tr41

0.8

0.55

0.7

0.50
0.6

0.40

H_W_RF
C4.5_W_RF

0.35

CART_W_RF

Strength

Strength

0.45
0.5
H_W_RF
C4.5_W_RF

0.4

CART_W_RF

CHAID_W_RF

0.30

CHAID_W_RF

H_RF

H_RF

0.3

C4.5_RF

0.25

C4.5_RF

CART_RF

CART_RF

0.2

CHAID_RF

CHAID_RF

0.20
10

20

30

40

50

60

70

80

90

100

10

20

30

40

50

60

70

80

90

100

Number of features

Number of features

Wap

Tr31

0.9

0.44
0.8

0.40

0.36

Strength

H_W_RF

0.28

C4.5_W_RF
CART_W_RF

0.24

Strength

0.7

0.32

0.6
H_W_RF
C4.5_W_RF

0.5

CART_W_RF
CHAID_W_RF

CHAID_W_RF
H_RF

0.20

H_RF

0.4

C4.5_RF

C4.5_RF

CART_RF

CART_RF

0.16

0.3

CHAID_RF

CHAID_RF

0.12
10

20

30

40

50

60

70

80

90

100

10

20

30

La2s

60

70

80

90

100

La1s

0.60

0.55

0.55

0.50

0.50

0.45

0.45
H_W_RF

0.40

C4.5_W_RF
CART_W_RF
CHAID_W_RF

0.35

Strength

Strength

50

Number of features

Number of features

0.60

40

H_W_RF

0.40

C4.5_W_RF
CART_W_RF

0.35

CHAID_W_RF

H_RF
C4.5_RF

0.30

H_RF

0.30

C4.5_RF

CART_RF
CHAID_RF

CART_RF

0.25

CHAID_RF

0.25
10

20

30

40

50

60

70

80

90

100

110

120

130

10

20

Number of features

30

40

50

60

70

80

90

100

110

120

130

Number of features

FIGURE 2. Strength changes against the number of features in the subspace on the 8 high dimensional datasets

Tr31) to run the random forest algorithms, and used
a subspace size of 120 features in the last 2 datasets
(La2s and La1s) to run these random forest algorithms.
For H W RF, C4.5 W RF, CART W RF, and
CHAID W RF, we used Breiman’s subspace size of

⌊log2 (M ) + 1⌋ to run these random forest algorithms.
This number of features provided a consistent result as
shown in Fig. 5. In order to obtain stable results, we
built 20 random forest models for each random forest
algorithm and each dataset and present the average

The Computer Journal, Vol. ??,

No. ??,

????

Hybrid weighted random forests for classifying very high-dimensional data
Fbis

9

Re0

0.216

0.285

0.208

0.270

Correlation

Correlation

0.255

0.200

0.240

0.192
H_W_RF
C4.5_W_RF

0.184

CART_W_RF
CHAID_W_RF

0.176

H_W_RF

0.225

C4.5_W_RF
CART_W_RF

0.210

CHAID_W_RF

H_RF
C4.5_RF

0.168

H_RF

0.195

C4.5_RF

CART_RF
CHAID_RF

CART_RF

0.180

CHAID_RF

0.160
10

20

30

40

50

60

70

80

90

100

10

20

30

Number of features

40

50

70

80

90

100

Number of features

Re1

0.27

60

Tr41
0.18

0.26

0.16

0.25

0.23
H_W_RF
C4.5_W_RF

0.22

CART_W_RF
CHAID_W_RF

0.21

Correlation

Correlation

0.24

0.14

H_W_RF

0.12

C4.5_W_RF
CART_W_RF

0.10

CHAID_W_RF
H_RF

H_RF
C4.5_RF

0.20

C4.5_RF

0.08

CART_RF

CART_RF

0.19

CHAID_RF

CHAID_RF

0.06
10

20

30

40

50

60

70

80

90

100

10

20

30

40

50

60

70

80

90

100

Number of features

Number of features

Wap

Tr31

0.27

0.14

0.26
0.12

Correlation

0.24

0.23

H_W_RF
C4.5_W_RF

0.22

CART_W_RF
CHAID_W_RF

0.21

Correlation

0.25

0.10

H_W_RF
C4.5_W_RF

0.08

CART_W_RF
CHAID_W_RF

0.06

H_RF

H_RF

C4.5_RF

0.20

C4.5_RF

CART_RF

CART_RF

0.04

CHAID_RF

0.19

10

20

30

40

50

60

70

80

90

100

CHAID_RF

10

20

30

40

50

60

70

80

90

100

Number of features

Number of features

La2s

La1s

0.162

0.165

0.156

0.160

0.155

0.150

H_W_RF

0.138

C4.5_W_RF
CART_W_RF

0.132

CHAID_W_RF

0.126

Correlation

Correlation

0.150

0.144

0.145

H_W_RF
C4.5_W_RF

0.140

CART_W_RF
CHAID_W_RF

0.135

H_RF

H_RF
C4.5_RF

0.120

C4.5_RF

0.130

CART_RF

CART_RF
CHAID_RF

CHAID_RF

0.125

0.114
10

20

30

40

50

60

70

80

90

100

110

120

130

10

20

Number of features

30

40

50

60

70

80

90

100

110

120

130

Number of features

FIGURE 3. Correlation changes against the number of features in the subspace on the 8 high dimensional datasets

results, noting that the range of values are less than
±0.005 and the hybrid trees are always more accurate.
The comparison results of classiﬁcation performance
of eleven methods are shown in Table 3.
The
performance is estimated using test accuracy (Acc),

Micro F1 (Mic), and Macro F1 (Mac). Boldface
denotes best results between eleven classiﬁcation
methods.
While the improvement is often quite
small, there is always an improvement demonstrated.
We observe that our proposed method (H W RF)

The Computer Journal, Vol. ??,

No. ??,

????

10

Baoxun Xu, Joshua Zhexue Huang, Graham Williams, Yunming Ye
Fbis

3.50

Re0
4.0

3.15

log (M)+1

3.5

2

2.80

3.0

2.45

C4.5_W_RF

1.75

2.5

2

H_W_RF

C/S

C/S

2

2.10

CART_W_RF

H_W_RF
C4.5_W_RF

2.0

CART_W_RF

CHAID_W_RF

1.40

CHAID_W_RF

1.5

H_RF

H_RF

log (M)+1
2

C4.5_RF

1.05

C4.5_RF

1.0

CART_RF

CART_RF

CHAID_RF

0.70

CHAID_RF

0.5
10

20

30

40

50

60

70

80

90

100

10

20

30

Number of features

70

80

90

100

3.5

4.9

log (M)+1

3.0

4.2

2

C/S

H_W_RF

2

2.5

3.5

2

60

Tr41

4.0

5.6

C/S

50

Number of features

Re1

6.3

40

C4.5_W_RF

2.8

2.0

H_W_RF
C4.5_W_RF

1.5

CART_W_RF

CART_W_RF

CHAID_W_RF

2.1

CHAID_W_RF

1.0

H_RF

H_RF

C4.5_RF

log (M)+1

1.4

2

C4.5_RF

0.5

CART_RF

CART_RF

CHAID_RF

0.7

CHAID_RF

0.0
10

20

30

40

50

60

70

80

90

100

10

20

30

40

50

60

70

80

90

100

Number of features

Number of features

Wap

Tr31

14

1.50

12

log (M)+1

log (M)+1
2

1.25

2

10

C4.5_W_RF

C/S

2

C/S

H_W_RF

6

2

1.00

8

H_W_RF

0.75

C4.5_W_RF
CART_W_RF

CART_W_RF
0.50

CHAID_W_RF

4

CHAID_W_RF
H_RF

H_RF
C4.5_RF

2

C4.5_RF

0.25

CART_RF

CART_RF
CHAID_RF

CHAID_RF

0

10

20

30

40

50

60

70

80

90

0.00

100

10

20

30

40

50

60

70

80

90

100

Number of features

Number of features

La1s

La2s
2.2

1.8

2.0
1.6
1.8
1.4
1.6
1.2

C4.5_W_RF
CART_W_RF

0.8

2

H_RF

0.6

C4.5_RF
CART_W_RF

0.4

CHAID_RF

20

30

40

2

H_W_RF

1.2

C4.5_W_RF
CART_W_RF

1.0

CHAID_W_RF

log (M)+1

10

C/S

C/S

2

1.4
H_W_RF

1.0

50

60

70

80

90

100

110

120

CHAID_W_RF

log (M)+1
2

0.8

H_RF
C4.5_RF

0.6

CART_RF
CHAID_RF

0.4

130

10

20

Number of features

30

40

50

60

70

80

90

100

110

120

130

Number of features

FIGURE 4. c/s2 changes against the number of features in the subspace on the 8 high dimensional datasets

outperformed the other classiﬁcation methods in all
datasets.

5.

CONCLUSIONS

In this paper, we presented a hybrid weighted random
forest algorithm by simultaneously using a feature
weighting method and a hybrid forest method to classify
The Computer Journal, Vol. ??,

No. ??,

????

Hybrid weighted random forests for classifying very high-dimensional data
Fbis

0.86

11

Re0

0.88

0.84
0.84
0.80

0.76

0.80
H_W_RF
C4.5_W_RF

0.78

CART_W_RF

Accuracy

Accuracy

0.82

CHAID_W_RF
H_RF

0.76

0.72
H_W_RF

0.68

C4.5_W_RF
CART_W_RF

0.64

CHAID_W_RF
H_RF

0.60

C4.5_RF

C4.5_RF

log (M)+1

CART_RF

2

0.74

CART_RF

log (M)+1

0.56

2

CHAID_RF

CHAID_RF

0.52
10

20

30

40

50

60

70

80

90

100

10

20

30

Number of features

40

50

60

70

80

90

100

Number of features

Re1

Tr41

1.00

0.86
0.95

0.84

log (M)+1

0.82

0.90

2

0.78
H_W_RF
C4.5_W_RF

0.76

CART_W_RF

0.74

Accuracy

Accuracy

0.80

CHAID_W_RF

0.85

H_W_RF

0.80

C4.5_W_RF
CART_W_RF

0.75

CHAID_W_RF
H_RF

H_RF

0.72

log (M)+1

0.70

C4.5_RF

C4.5_RF

2

CART_RF

CART_RF

0.70

0.65

CHAID_RF

CHAID_RF

0.68
10

20

30

40

50

60

70

80

90

100

10

20

30

50

60

70

80

90

100

Number of features

Number of features

Wap

0.84

40

Tr31

1.000

0.81

0.975

0.78

0.950

0.75

H_W_RF
C4.5_W_RF

0.69

CART_W_RF

Accuracy

Accuracy

0.925

0.72

0.900

H_W_RF
C4.5_W_RF

0.875

CART_W_RF
CHAID_W_RF

CHAID_W_RF

0.66

H_RF

log (M)+1
2

0.850

H_RF

C4.5_RF

0.63

CART_RF

0.60

C4.5_RF

log (M)+1
2

0.825

CART_RF
CHAID_RF

CHAID_RF

0.800
10

20

30

40

50

60

70

80

90

100

10

20

30

50

60

70

80

90

100

Number of features

Number of features

La2s

0.900

40

La1s

0.88

0.885
0.86
0.870

Accuracy

0.840
H_W_RF
C4.5_W_RF

0.825

CART_W_RF
CHAID_W_RF

0.810

Accuracy

0.84

0.855

0.82

H_W_RF
C4.5_W_RF
CART_W_RF

0.80

CHAID_W_RF
H_RF

H_RF

log (M)+1

0.795

C4.5_RF

2

C4.5_RF

log (M)+1

0.78

2

CART_RF

CART_RF
CHAID_RF

CHAID_RF

0.780

0.76
10

20

30

40

50

60

70

80

90

100

110

120

130

10

20

Number of features

30

40

50

60

70

80

90

100

110

120

130

Number of features

FIGURE 5. Test Accuracy changes against the number of features in the subspace on the 8 high dimensional datasets

high dimensional data. Our algorithm not only retains
a small subspace size (Breiman’s formula ⌊log2 (M ) + 1⌋
for determining the subspace size) to create accurate
random forest models, but also eﬀectively reduces
the upper bound of the generalization error and

improves classiﬁcation performance. From the results of
experiments on various high dimensional datasets, the
random forest generated by our new method is superior
to other classiﬁcation methods. We can use the default
⌊log2 (M ) + 1⌋ subspace size and generally guarantee

The Computer Journal, Vol. ??,

No. ??,

????

12

Baoxun Xu, Joshua Zhexue Huang, Graham Williams, Yunming Ye

TABLE 3. The comparison of results
datasets
Dataset
Fbis
Measures
Acc
Mic
SVM
0.834 0.799
KNN
0.78
0.752
NB
0.776 0.74
H RF
0.853 0.816
C4.5 RF
0.836 0.806
CART RF
0.829 0.797
CHAID RF
0.842 0.805
H W RF
0.856 0.825
C4.5 W RF
0.841 0.809
CART W RF
0.835 0.805
CHAID W RF
0.839 0.815
Dataset
Wap
Measures
Acc
Mic
SVM
0.81
0.772
KNN
0.752 0.622
NB
0.797 0.742
H RF
0.815 0.805
C4.5 RF
0.797 0.795
CART RF
0.793 0.793
CHAID RF
0.805 0.805
H W RF
0.815 0.805
C4.5 W RF
0.805 0.795
CART W RF
0.8
0.792
CHAID W RF
0.811 0.795

(best accuracy, Micro F1, and Macro F1 results) of the eleven methods on the 8
Re0
Mic
0.795
0.752
0.741
0.82
0.802
0.798
0.8
0.825
0.815
0.81
0.812
Tr31
Mac
Acc
Mic
0.663 0.955 0.907
0.622 0.905 0.82
0.559 0.925 0.832
0.735 0.965 0.925
0.732 0.962 0.902
0.73
0.958 0.892
0.732 0.96
0.9
0.735 0.965 0.925
0.732 0.962 0.911
0.73
0.96
0.902
0.73
0.96
0.905

Mac
0.76
0.722
0.706
0.816
0.806
0.787
0.805
0.82
0.815
0.81
0.812

Acc
0.804
0.779
0.784
0.845
0.836
0.826
0.832
0.855
0.845
0.839
0.842

to always produce the best models, on a variety of
measures, by using the hybrid weighted random forest
algorithm.
ACKNOWLEDGEMENTS
This research is supported in part by NSFC under
Grant NO.61073195, and Shenzhen New Industry Development Fund under Grant NO.CXB201005250021A
REFERENCES
[1] Breiman, L. (2001) Random forests. Machine learning,
45, 5–32.
[2] Ho, T. (1998) Random subspace method for constructing decision forests. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 20, 832–844.
[3] Quinlan, J. (1993) C4.5: Programs for machine
learning. Morgan Kaufmann.
[4] Breiman, L. (1984) Classification and regression trees.
Chapman & Hall/CRC.
[5] Breiman, L. (1996) Bagging predictors.
Machine
learning, 24, 123–140.
[6] Ho, T. (1995) Random decision forests. Proceedings
of the Third International Conference on Document
Analysis and Recognition, pp. 278–282. IEEE.
[7] Dietterich, T. (2000) An experimental comparison of
three methods for constructing ensembles of decision
trees: Bagging, boosting, and randomization. Machine
learning, 40, 139–157.

Mac
0.756
0.752
0.619
0.82
0.802
0.798
0.8
0.822
0.812
0.805
0.815
Mac
0.87
0.762
0.81
0.88
0.87
0.86
0.852
0.88
0.87
0.865
0.855

Re1
Mic
0.826
0.668
0.732
0.832
0.811
0.808
0.815
0.836
0.826
0.818
0.83
la2s
Acc
Mic
0.89
0.832
0.841 0.805
0.896 0.815
0.89
0.84
0.878 0.83
0.882 0.832
0.88
0.83
0.896 0.848
0.886 0.835
0.887 0.835
0.887 0.833
Acc
0.829
0.788
0.816
0.841
0.825
0.825
0.838
0.848
0.838
0.835
0.84

Tr41
Mic
0.915
0.813
0.856
0.926
0.92
0.891
0.903
0.926
0.922
0.91
0.915
la1s
Mac
Acc
Mic
0.807 0.875 0.82
0.786 0.827 0.798
0.79
0.87
0.802
0.82
0.862 0.825
0.81
0.855 0.82
0.81
0.84
0.815
0.803 0.845 0.816
0.825 0.875 0.836
0.816 0.866 0.825
0.812 0.87
0.825
0.81
0.865 0.825
Mac
0.706
0.638
0.58
0.8
0.781
0.783
0.795
0.81
0.795
0.79
0.8

Acc
0.95
0.915
0.935
0.953
0.948
0.917
0.926
0.953
0.95
0.935
0.942

Mac
0.87
0.765
0.782
0.895
0.89
0.88
0.88
0.895
0.892
0.88
0.88
Mac
0.803
0.761
0.775
0.805
0.798
0.792
0.795
0.82
0.81
0.81
0.805

[8] Banfield, R., Hall, L., Bowyer, K., and Kegelmeyer, W.
(2007) A comparison of decision tree ensemble creation
techniques. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 29, 173–180.
[9] Robnik-Šikonja, M. (2004) Improving random forests.
Proceedings of the 15th European Conference on
Machine Learning, pp. 359–370. Springer.
[10] Ho, T. (1998) C4.5 decision forests. Proceedings of
the Fourteenth International Conference on Pattern
Recognition, pp. 545–549. IEEE.
[11] Dietterrich, T. (1997) Machine learning research: four
current direction. Artificial Intelligence Magzine, 18,
97–136.
[12] Amaratunga, D., Cabrera, J., and Lee, Y. (2008)
Enriched random forests. Bioinformatics, 24, 2010–
2014.
[13] Ye, Y., Li, H., Deng, X., and Huang, J. (2008)
Feature weighting random forest for detection of hidden
web search interfaces. The Journal of Computational
Linguistics and Chinese Language Processing, 13, 387–
404.
[14] Xu, B., Huang, J., Williams, G., Wang, Q., and
Ye, Y. (2012) Classifying very high-dimensional data
with random forests built from small subspaces.
International Journal of Data Warehousing and
Mining, 8, 45–62.
[15] Xu, B., Huang, J., Williams, G., Li, J., and Ye, Y.
(2012) Hybrid random forests: Advantages of mixed
trees in classifying text data. Proceedings of the 16th
Pacific-Asia Conference on Knowledge Discovery and
Data Mining. Springer.

The Computer Journal, Vol. ??,

No. ??,

????

Hybrid weighted random forests for classifying very high-dimensional data
[16] Biggs, D., De Ville, B., and Suen, E. (1991) A method
of choosing multiway partitions for classification and
decision trees. Journal of Applied Statistics, 18, 49–62.
[17] Ture, M., Kurt, I., Turhan Kurum, A., and Ozdamar,
K. (2005) Comparing classification techniques for
predicting essential hypertension. Expert Systems with
Applications, 29, 583–588.
[18] Begum, N., M.A., F., and Ren, F. (2009) Automatic text summarization using support vector machine.
International Journal of Innovative Computing, Information and Control, 5, 1987–1996.
[19] Chen, J., Huang, H., Tian, S., and Qu, Y. (2009)
Feature selection for text classification with naive
bayes. Expert Systems with Applications, 36, 5432–
5435.
[20] Tan, S. (2005) Neighbor-weighted k-nearest neighbor
for unbalanced text corpus.
Expert Systems with
Applications, 28, 667–671.
[21] Pearson, K. (1904) On the Theory of Contingency and
Its Relation to Association and Normal Correlation.
Cambridge University Press.
[22] Yang, Y. and Liu, X. (1999) A re-examination of
text categorization methods. Proceedings of the 22th
International Conference on Research and Development
in Information Retrieval, pp. 42–49. ACM.
[23] Han, E. and Karypis, G. (2000) Centroid-based
document classification: Analysis and experimental
results. Proceedings of the 4th European Conference on
Principles of Data Mining and Knowledge Discovery,
pp. 424–431. Springer.
[24] TREC.
(2011)
Text
retrieval
conference,
http://trec.nist.gov.
[25] Lewis,
D.
(1999)
Reuters-21578
text
categorization
test
collection
distribution
1.0,
http://www.research.att.com/ lewis.
[26] Han, E., Boley, D., Gini, M., Gross, R., Hastings,
K., Karypis, G., Kumar, V., Mobasher, B., and
Moore, J. (1998) Webace: A web agent for document
categorization and exploration. Proceedings of the 2nd
International Conference on Autonomous Agents, pp.
408–415. ACM.
[27] McCallum, A. and Nigam, K. (1998) A comparison of
event models for naive bayes text classification. AAAI98 workshop on learning for text categorization, pp. 41–
48.
[28] Witten, I., Frank, E., and Hall, M. (2011) Data Mining:
Practical machine learning tools and techniques.
Morgan Kaufmann.

The Computer Journal, Vol. ??,

No. ??,

????

13

