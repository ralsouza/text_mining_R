ReDSOM: Relative Density Visualization of Temporal Changes
in Cluster Structures using Self-Organizing Maps
Denny
Department of Computer Science, The Australian National University, Canberra, Australia
Faculty of Computer Science, University of Indonesia, Indonesia
denny@cs.anu.edu.au, denny@cs.ui.ac.id
Graham J. Williams
Australian Taxation Office, Canberra, Australia
graham.williams@ato.gov.au
Peter Christen
Department of Computer Science, The Australian National University, Canberra, Australia
peter.christen@anu.edu.au

Abstract
We introduce a Self-Organizing Map (SOM) based visualization method that compares cluster structures in temporal datasets using Relative Density SOM (ReDSOM)
visualization. Our method, combined with a distance
matrix-based visualization, is capable of visually identifying emerging clusters, disappearing clusters, enlarging
clusters, contracting clusters, the shifting of cluster centroids, and changes in cluster density. For example, when
a region in a SOM becomes significantly more dense compared to an earlier SOM, and well separated from other
regions, then the new region can be said to represent a new
cluster. The capabilities of ReDSOM are demonstrated using synthetic datasets, as well as real-life datasets from the
World Bank and the Australian Taxation Office. The results
on the real-life datasets demonstrate that changes identified
interactively can be related to actual changes. The identification of such cluster changes is important in many contexts, including the exploration of changes in population behavior in the context of compliance and fraud in taxation.

1. Introduction
Businesses and government organizations need knowledge of change in order to adapt their strategies to everchanging environments. Knowing what has changed can be
a major competitive advantage for an organization. To un-

derstand what has changed, analysts have to be able to relate
new knowledge or models acquired from a newer dataset to
those acquired from an earlier dataset. Without this context, it can be difficult to revise existing strategies. This is
particularly problematic if an organization has already implemented a strategy based on an earlier model.
In supervised learning, classifier performance often degrades over time, an issue known as concept drift [19, 23].
In many real-life domains, a concept of interest may depend on some hidden context, which is not given explicitly
in the form of predictive features (i.e. some variables are
invisible to the learner). For example, such hidden concepts
can be changes in economic policy, disasters, life events,
or changes in marketing strategies. Changes in the hidden
context can induce more or less radical changes in a target
concept. Most research in concept drift only addresses concept drift in a supervised learning context—little has been
researched in the context of unsupervised learning.
In data mining of conceptual changes, a number of temporal data mining algorithms have focused on detecting the
point in time when something has changed (change detection), rather than understanding or exploring the causes that
have made the changes (change analysis). For example, by
gradually eliminating the effects of past data, an on-line
discounting learning algorithm can detect outliers and the
change points in time in a changing data source [25].
To discover changes between two datasets, the resulting
data mining models can be compared, given that a data mining model is designed to capture specific characteristics of a
dataset. A theoretical framework has been introduced in [7]
that allows measuring changes between two models. In this

framework, when the structural components of the models
are different, both structures are ‘extended’ to a greatest
common refinement. The deviation between two models is
then calculated by aggregating the differences in the measurement components of the models.
This paper focuses on visualizing, identifying, and analyzing changes in cluster structures in a SOM, trained with
a newer dataset, compared to a SOM trained with an older
dataset. Temporal cluster analysis can be useful to understand changes in datasets, or to review the effectiveness of
deployed strategies. For example, if an organization has devised a marketing strategy based on a clustering of the past
year’s customer data, it is important to know if the current
year’s clustering differs, in order to understand changes in
customer behavior, or to review the effectiveness of the implemented marketing strategy. New strategies can then be
devised to encourage or deter the development of new clusters or to slow the demise of clusters, as suits the requirements of the business.
ReDSOM visualization allows users to explore the distinctive features of changes interactively using the hot-spot
methodology [6]. Involving the user in the data exploration
process is important in ensuring effective data analysis [12].
We propose the use of SOMs for effectively visualizing
changes. SOMs have several advantages in temporal cluster analysis. They are able to relate clustering results by
linking multiple visualizations, and they can detect various
types of cluster changes, including emerging and disappearing clusters [5]. Furthermore, SOMs create a smaller but
representative dataset, and they have topology preservation
properties [15]. Importantly, SOMs can be used to explore
high-dimensional data spaces through a non-linear projection onto a two-dimensional (2-D) plane using visualizations that are easy to understand even by non-analysts [15].
A mathematical analysis of SOM properties can be found
in [17]. Applications of SOM for data mining are found in
engineering, speech analysis and recognition, finance, and
information retrieval [4, 15].
We contrast our ReDSOM visualization methodology to
the goals of time series clustering [13]. Time-series clustering aims to cluster entities that have similar time-series
patterns, whereas our method clusters entities at points in
time (snapshots), and compares the clustering structures of
such snapshots.
Research in data stream mining has also provided some
insights into the problem described here. Aggarwal et.al. [2]
presented a framework for clustering data streams where the
aim is to discover changes in the evolving data streams. The
basic idea of the approach is to divide clustering into an online and off-line component, with the online component periodically storing summary information of the data stream
clusters (so called micro-clusters), and the off-line component generating aggregated clusters according to the needs

of an analyst. Our work does not consider data streams but
investigates snapshot datasets that were collected at different points in time. While data streams are becoming common in many application areas, static snapshot datasets are
still the most commonly used type of data in many organizations. There still remains a need to analyze changes
between such snapshots.
The main contribution of this paper is the development
of Relative Density SOM (ReDSOM) visualization that can
compare and contrast changes in cluster structures in temporal datasets. ReDSOM allows analysts to explore and
understand changes interactively. We evaluate our method
on synthetic datasets and on real-life datasets published by
the World Bank [24]. We have also evaluated our method
on large real-life datasets from the Australian Taxation Office. The results on the real-life datasets demonstrate that
changes identified interactively can be related to actual reallife changes.
The remainder of the paper is organized as follows. The
next section discusses related work in temporal cluster analysis. An overview of Self-Organizing Maps is provided in
Section 3. Section 4 introduces our relative density definition and ReDSOM visualization. The results of our experiments are then discussed in Section 5, and conclusions and
future work are provided in Section 6.

2. Related Work
Temporal data can be grouped into four broad categories:
static, sequences, time-stamped, and fully temporal [18]. In
static datasets, temporal context is not included and cannot
be inferred. Sequences are basically ordered lists of events,
but not time-stamped. Examples of time-stamped datasets
are census data, web-based activity, or sales transaction. In
fully temporal databases, each tuple in a time-varying relation in the database may have one or more dimensions of
time, such as age and treatment time. Our method analyzes
changes in two or more static temporal datasets.
Chakrabarti, Kumar, and Tomkins [3] defined evolutionary clustering as the problem of processing time-stamped
data to produce a sequence of clusterings; that is, a clustering for each time step. This framework tries to optimize two potentially conflicting criteria: remaining faithful to the current data, and not shifting dramatically from
the previous clustering results. Therefore, the user has to
define a snapshot quality function sq(Ct , Dt ) which measures the quality of a clustering result Ct for dataset Dt
at time t, and a history cost function hc(Ct−1 , Ct ) which
measures how much a latter clustering result Ct differs
from the previous one, Ct−1 . The optimal cluster sequence
can, therefore, be found by determining at each time step
t a clustering Ct that optimizes the incremental quality
sq(Ct , Dt ) − cp · hc(Ct−1 , Ct ), where cp is a non-negative

change parameter. As cp is increased, more weight is placed
on matching the historical clusters. Based on this, the authors derived an agglomerative hierarchical and a k-means
clustering algorithm. When calculating clustering result Ct
using k-means, for example, the previous cluster centroids
of Ct−1 are used as the starting seeds. The new centroids
are then calculated based on the closest match of cluster
centroids in the previous clustering result, and the cluster
centroids of the non-evolutionary (conventional) k-means.
This framework is able to find a balance between remaining faithful and not shifting dramatically in training the subsequent clustering results, in order to smooth the clustering
sequence. However, the aim of this framework is not to analyze the changes of the clustering results. It is not easy to
understand the actual cluster changes using plots of snapshot quality and historical cost over time. It is not clear how
to relate and understand changes in terms of the earlier clustering results. Furthermore, the capability of the framework
to detect any rate of changes (abrupt or gradual changes) is
questioned as the change parameter cp is constant over time
and has to be defined beforehand.
The k-means version of the framework also has some issues related to the discovery of new or lost clusters. With
the proposed k-means variant, the previous cluster centroids
Ct−1 are used as starting seeds. A problem arises when
there is a larger number of cluster (k) selected in finding
the latter clustering result Ct . It is not clear how to initialize the additional cluster centroids in this case. A similar
problem occurs when a smaller k is selected. In this case,
one or more cluster centroids from the previous clustering
result Ct−1 have to be removed in finding the latter clustering result Ct . Our method, on the other hand, is able to
show emerging clusters and lost clusters without having to
determine the number of clusters beforehand.
Hido et.al. [9] proposed an approach to explain changes
between two datasets by using a decision tree, and labeling one dataset as negative and the other as positive. The
trained model is then investigated to understand differences
between the datasets. As decision trees divide the data space
into hypercubes and try to separate the entities based on
their labels, this approach can detect when there is a new
hypercube that was more sparsely occupied by the other
dataset. However, this method cannot show the separation between the hypercubes and the density of these hypercubes. Therefore, this method cannot differentiate between
emerging clusters and cluster enlargements. In our method,
on the other hand, separation between prototype vectors can
be shown using distance matrix visualization [10]. Furthermore, when correlated attributes exist in the dataset, only
one of them will be used to describe new or lost hypercubes,
reducing the description of the hypercubes.
Recently, Adomavicius and Bockstedt [1] introduced a
graph visualization technique for exploring trends in multi-

attribute temporal datasets using a temporal cluster graph.
In this technique, transactional dataset D is partitioned into
data subsets Dt according to time periods. Each data subset is then clustered. Only clusters that have at least α|Dt |
number of entities are shown as nodes, where α ∈ [0, 1]
is a node filter parameter. Nodes between two adjacent
time periods are connected with an edge, if the distance between the nodes is less than a threshold η, which is calculated based on an edge filter parameter β and the average
between-cluster distances. This graph is visualized interactively, where users have to experiment with the number of
clusters for each partition Dt , node filter parameter α, and
edge filter parameter β. In our method, users do not have to
experiment to find the optimal number of clusters.
For detecting changes between two SOMs, Kaski and
Lagus [11] proposed a dissimilarity measure for two maps,
which is calculated based on the expected value of distances
between pairs of representative data points on both maps.
This approach can determine how much two SOMs differ,
but it cannot analyze the changes.
Lingras et.al. [16] studied the temporal cluster characteristics of supermarket customers using an interval set basedSOM. The capability of the proposed method to detect new
clusters is questioned as the number of clusters was constant
for all time periods in their experiments.
Denny and Squire [5] proposed a SOM training method
and SOM-based visualization techniques that are capable of
explaining the clustering results of a second dataset in terms
of the clustering result of a first dataset by using color and
position linking. These visualization techniques can relate
two clustering results which can show the following structural changes: changes in cluster size, centroid movements,
new clusters, cluster splitting, missing clusters, and cluster merging. Such changes have to be analyzed visually
by selecting a number of clusters for both datasets. However, these techniques cannot differentiate between cluster
enlargement (occupying new space), and increase in cluster
density (more entities in the same space).

3. Self-Organizing Maps
A SOM is an artificial neural network that performs unsupervised competitive learning [14]. Artificial neurons
are arranged on a low-dimensional grid, commonly a 2D plane with nr rows, nc columns, and a total number of
nunit = nr · nc units. Each neuron j has an d-dimensional
prototype vector, mj , where d is the dimensionality of
dataset D. Each neuron is connected to neighboring neurons, with distances to its neighbours being equidistant in
the map space. The lattice structure can be a hexagonal grid,
where each neuron is connected to six neighbours. Larger
maps generally have higher accuracy and generalization capability [22], but they also have higher computation costs.

Before training a map, the prototype vectors should be
initialized using random initial values, or ordered values
(linear initialization) [15]. Linear initialization uses ordered
values of component vectors based on the first two largest
principal components. When using random initialization,
the radius of the neighbourhood function should be large
enough; otherwise the map will not be globally ordered.
However, if linear initialization is used for the initial map,
then a smaller radius and a shorter training length could be
used [15]. Therefore, linear initialization is preferred over
random initialization, because it can speed up the learning
process by orders of magnitude [15].
At each training step t, the best matching unit bi (BMU)
for training data vector xi , i.e. the prototype vector mj closest to the training data vector xi , is selected from the map
according to Equation 1:
∀j,

kxi − mbi (t)k ≤ kxi − mj (t)k

(1)

In the batch training algorithm [15], the values of new
prototype vectors mj (t + 1) are weighted averages of the
training data vectors xi , where the weight is the neighbourhood kernel value hbi j centered on the best matching unit
bi . Since the neighbourhood function hbi ,j value is the same
for all data vectors mapped to the same unit, the sum SVj
of the Voronoi set Vj of each prototype vector mj at training step t. So, each training data vector belonging to the
Voronoi set of its closest prototype vector (BMU), can be
calculated first using Equation 2:
SVj (t) =

X

xi .

(2)

xi ∈Vj

Then, the prototype vectors are updated with:
Pnunit
i=1 hji (t) · SVi (t)
,
mj (t + 1) = P
nunit
i=1 hji (t) · nVi

show the spread of values of a certain component of all prototype vectors in a SOM [21]. Distance-matrix based visualizations, such as u-matrix visualization [10], show distances between neighboring nodes using a color scale representation on a map grid. This visualization can be used
to identify borders between clusters, where long distances
show highly dissimilar features between neighboring nodes
that divide clusters, i.e. the dense parts of a map with similar
features [10].
In [22], the prototype vectors of a trained SOM can be
treated as ‘proto-clusters’ serving as an abstraction of the
dataset. The prototype vectors are then clustered using a
traditional clustering technique, such as k-means, to form
the final clusters. In this two-level clustering, adding an
extra layer simplifies the clustering task and reduces noise,
but may yield higher distortion [22].

4. Relative Density
Let D(τ1 ) be a dataset at time τ1 , and D(τ2 ) be a dataset
at time τ2 , where τ1 < τ2 . In order to be able to compare
two maps that are trained using datasets D(τ1 ) and D(τ2 ),
the orientation of map M (τ1 ) and M (τ2 ) must be the same.
Therefore, the following map training procedure, as proposed in [5], is used.
1. Normalize both datasets D(τ1 ) and D(τ2 ) using the
same normalization method (e.g. z-score) and parameters (e.g. the same mean and standard deviation values) for the same attributes.
2. Initialize map M (τ1 ) using ordered values.
3. Train map M (τ1 ) using dataset D(τ1 ).

(3)

where hji is the neighbourhood kernel function centered on
unit j (commonly Gaussian) [15], and nVj is the number of
training data vectors in Voronoi set Vj . To handle missing
values, SVj and nVj only perform summation and counting of non-missing components, respectively. This calculation of new prototype vectors takes into account neighboring prototype vectors which preserve the topological order.
The map is usually trained in two phases: a rough training phase and a fine tuning phase. The rough training phase
usually has shorter training length and larger initial radius
compared to fine tuning phase [15].
SOMs are popularly used in cluster analysis because they
perform vector quantization and preserve topological order. Furthermore, the trained SOMs can be visualized using
various methods that allow non-technical users to explore
a dataset. Component plane visualizations can be used to

4. Initialize map M (τ2 ) using the prototype vectors of the
trained map M (τ1 ).
5. Train map M (τ2 ) using dataset D(τ2 ).

4.1. Relative Density Definition
As a SOM follows the distribution of a dataset it is
trained on, more prototype vectors are allocated for dense
regions, as shown in Figure 1. Therefore, area density at
mj (τ1 ) on its own map, M (τ1 ), might be different compared to area density at the same location on map M (τ2 ).
When the area density at the location of the prototype vector mj (τ1 ) in D(τ2 ) is more sparse, the area density on map
M (τ2 ) is lower, compared to the area density at the same location on map M (τ1 ), and vice-versa. In Figure 1, the area
density at the marked area (the center of the Gaussian kernel contour) on the left plot is higher, compared to the area
density at the same location on the right plot.

2.5

2.5
C

2

1.5
B

1
0.

0
X

1

data vector

2
prototype vector

0.1

0.2

0.1

0.2

0.8

0.4

-1

D

0.2

0.
1

-2

0.
1

0 .2

Y

-2

0.1

-1.5
-2

E

0.4

D

0.
1

-2

0.4

-1

0.2

-1.5

0.6

0.4

-1

-0.5

0.6

A

0.8

0
0.2

A

0.6

0.2

-0.5

0.5

E

0.6
0.4

0

0.2

0.1

0.2

0.5

B

1
0.2

1

0.1

0.4

0.1

Y

1.5

C

2

-1

0
X

1

2

Gaussian kernel function

Figure 1. Plots of data vectors and prototype vectors of the maps trained using two synthetic
datasets D(τ1 ) (left) and D(τ2 ) (right), where there is an emerging cluster (‘E’), a lost cluster (‘A’),
a more dense cluster (‘B’), a less dense cluster (‘D’), and an unchanged cluster (‘C’), in the dataset
D(τ2 ). The contour of the Gaussian kernel function centered on one of the cluster ‘A’ prototype
vectors is shown on both plots.

We define area density ρM (τ) (v) at the location of a vector v on map M (τ) as the weighted sum of similar prototype vectors mj (τ) on M (τ) centered on vector v, where
the weight is calculated based on a Gaussian kernel function centered on vector v, as shown in Equation 4:

ρM (τ) (v) =

X



kv − mj (τ)k
exp −
2·r

(4)

j=1,...,nunit

As the radius r can be different for different maps (e.g.
datasets with large attribute value ranges will need to have
a larger radius), the radius should be determined based on
“between neighbour distances” on the map. To adapt the
radius for different maps, the quartile (e.g. third quartile)
of these distances is used as the radius. As the area density is normalized into a relative density (Equation 5), this
relative density is not sensitive to the radius. However, the
radius should not be too small or too large. If the radius is
too small, the relative density will be too sensitive to noise.
On the other hand, if the radius is too large, then relative
density cannot capture the details of changes.
We define relative density RDM (τ2 )/M (τ1 ) (v) as the ratio
of the area density at location of vector v on map M (τ2 ) to
the area density at the same location on the reference map
M (τ1 ), as shown in Equation 5:


RDM (τ2 )/M (τ1 ) (v) = log2

ρM (τ2 ) (v)
ρM (τ1 ) (v)


(5)

Without using a logarithm function in Equation 5, values
between 0 and 1 are interpreted as becoming more sparse,
the value of 1 is interpreted as no change, and values above
1 are interpreted as becoming more dense. Therefore, a base
two logarithm is used to make it easier to interpret the ratio where positive values are interpreted as becoming more
dense, negative values as becoming more sparse, and 0 is
interpreted as no change. For example, a value of +2 is interpreted as the area centered at the location of vector v in
the dataset D(τ2 ) being four times more dense compared to
the same area in the reference dataset D(τ1 ).
Based on our observations, when the value of
RDM (τ2 )/M (τ1 ) (v) is less than −3, then the space at location of vector v is no longer occupied in the next map
M (τ2 ) (it is lost). Similarly, when RDM (τ2 )/M (τ1 ) (v) is
greater than +3, then the space at location of vector v was
not occupied on the reference map M (τ1 ). In other words,
the space is only occupied on map M (τ2 ).
Both Equations 4 and 5 are performed on all the prototype vectors of both maps, but not on the actual data vectors.
Therefore, the running time of the calculation for a map is
quadratic in the number of map units nu , not in the number
of data vectors nD(τ) , where nu  nD(τ) .

4.2. Relative Density Visualization
As a shorthand, let rd1 ← RDM (τ2 )/M (τ1 ) (mj (τ1 )) and
rd2 ← RDM (τ2 )/M (τ1 ) (mj (τ2 )).
To visualize the relative density of the locations of all
prototype vectors mj (τ1 ), the values of rd1 are visualized
on a map M (τ1 ) in a gradation of blue for positive values
and in a gradation of red for negative values1 , as shown in
the left map in Figure 2. Values over +3 are represented
as dark blue, and values under −3 are represented as dark
red. A value of 0 is represented as white, as it indicates no
change in the density. For example, visualizations of the
datasets and prototype vectors from Figure 1 can be seen in
Figure 2.
However, rd1 rarely returns values greater than +3, because all prototype vectors mj (τ1 ) are always present on
the reference map M (τ1 ) (prototype vector mj (τ1 ) is part
of map M (τ1 )). Therefore, ρM (τ1 ) (mj (τ1 )) at least returns 1, which makes the denomination part of Equation 5
larger compared to ρM (τ1 ) (mj (τ2 )). Therefore, values of
rd2 should be visualized on map M (τ2 ) to detect emerging
clusters, as shown in the right map in Figure 2. To detect
new clusters, rd2 is used because the area at the location
of prototype vector mj (τ2 ) might be empty on map M (τ1 ).
Similarly, rd2 cannot be used to detect lost clusters on map
M (τ2 ), because the empty space at the location of the prototype vector mj (τ1 ) is not represented on map M (τ2 ), as
M (τ2 ) follows the distribution of dataset D(τ2 ).
Because of SOM’s vector quantization property and our
definition of relative density, prototype vectors on map
M (τ1 ) that have negative rd1 values will be less represented on map M (τ2 ). For example, there are less prototype
vectors in cluster ‘D’ in the right map in Figure 2. On the
other hand, prototype vectors on map M (τ1 ) that have positive rd1 values will be more represented on map M (τ2 ), as
shown in cluster ‘B’ in the right map in Figure 2.

4.3. Analysis of Changed Regions
There are several types of possible structural changes between two datasets: new clusters, lost clusters, cluster enlargements, cluster contractions, shifting of centroids, and
changes in cluster density. All these structural changes can
be identified using ReDSOM and distance matrix visualizations. As mentioned before, distance-matrix based visualizations show distances between neighboring nodes using a
color scale representation on the map, which can be used
to identify cluster borders [10]. For example, in Figure 3,
there are four clusters in both datasets (light yellow regions)
separated by long distances.
1 The

SOM visualizations presented in this paper unfortunately require
a diverging color scheme to illustrate the relative density that is hard to
distinguish between positive and negative values using gray scale. We hope
the reader has access to the color version, at least can view the PDF file.

Identifying new clusters. New clusters in dataset D(τ2 )
can be identified by dark blue regions (values above +3)
on relative density visualization rd2 of map M (τ2 ), and
they have long distances at the border of the regions on
map M (τ2 ), for example cluster ‘E’, as shown in the right
maps in Figures 2 and 3. When a new cluster appears inside
the distribution of dataset D(τ1 ), in other words between
disjoint clusters, the values of rd1 are close to +3, and the
cluster is positioned in a sparse area (long distances in the
distance matrix visualization between the clusters). This is
due to interpolative units, which appear when the data clusters are disjoint [22], as can be seen in both plots in Figure 1.
Therefore, the area density at this gap is higher, compared
to the area density at the empty space outside the data distribution.
Identifying cluster enlargements, cluster contractions,
movement of cluster centroids. When a new dense area
emerges in dataset D(τ2 ), but it does not have a good separation to its neighbour, the changes can be interpreted as
cluster enlargements. This can be identified by dark blue
regions on the relative density visualization rd2 , and the
region has short distances at the border of the regions on
map M (τ2 ), as shown in the bottom-right corner of the right
maps in Figure 4. When this kind of new region appears at
the border of map M (τ2 ), it can be said that the changes
move towards the tail of the data distribution. Similarly,
cluster contraction can be identified by a lost region, but it
does not have a good separation to its neighbours. Movement of cluster centroids can be identified by simultaneous
cluster enlargements and cluster contractions.
Identifying lost clusters. Lost clusters in dataset D(τ1 )
can be identified by dark red regions (value below -3) on
the relative density visualization rd1 of map M (τ1 ), and
long distances at the border of the regions on map M (τ1 ).
An example is cluster ‘A’ in the left maps in Figures 2 and 3.
Identifying change of cluster density. An increase of
cluster density in dataset D(τ2 ) can be identified in the relative density visualization as light blue, for example cluster
‘B’ as shown in the left map in Figure 2. On the other hand,
a decrease of cluster density in dataset D(τ2 ) can be identified in the relative density visualization as light red, for
example cluster ‘D’ as shown in the left map in Figure 2.
Analyzing interesting changes. Once a region of interest
is selected interactively by a user, our hot spot methodology [6] can be used to understand distinctive features of
these changing regions. In this methodology, the component planes are sorted by the importance of the attributes
that distinguish the region from the rest of the population

Relative densityy of map-lostNewCluster1
0

8

1

16

9

D

2

10

3

4

18

25

26

19

20

13

14

7

32

40

33

34

48

41

42

Relative
ve density of map-lostNewCluster2
map-lostN

56

49

50

64

57

58

72

65

66

80

73

74

3.0000

88

81

82

A

89

0

2.0000

90

8

1

2

16

10

24

D

9

17

18

32

25

26

40
0

33

34

48

41

42

56

49

50

64

57

58

72

65

66

80
8

73

74

3.0000

88

A

81

82

89

27

28

21

22

15

35

36

29

30

23

43

44

37

38

52
5

45

46

C

31

51

39

59

60

53

54

47

67

68

61

62
6

55

75

76

69

70

63
6

83

84

77

78

B

71

79

3

0.0

92

86

1.0000

91

85

93
3

4

11

12

5

-1.0000

94

6

95

19

20

13

7

27

28

21

E

14

22

-2.000
87

2.0000

90

1.0000

12

6

17

11

5

24

15

35

36

29

30

23

43
3

44

37

38

31

51

52

45

46

60

53

54

C

39

59

47

67

B

68

61

62

75

76

69

70
7
0

83

84

77

78

91

0.0

92

85

86

93

-1.0000

94

-2.000
55

63

71

79

87

95

-3.000

-3.000

Figure 2. Relative density visualizations rd1 (left) and rd2 (right) of the datasets and maps shown in
Figure 1. The bar chart inside each node shows the value of components of the prototype vectors
(dark gray for component ‘X’ and light gray for component ‘Y’).

Distance
e matrix of map-lostNewCluster1
map lost
0

8

1

16

9

D

2

10

3

4

18

25

26

19

20

13

14

7

32

40

33

34

48

41

42

Distance matrix of map-lostN
map-lostNewCluster2

56

49

50

64

57

58
58

72

65

66

80

73

74

0.9276

88

A

81

8
82

89

0

0.6384

90

8

1

2

16

9

10

24

17

18

D

32

25

26

40

33

34

48

41

42

56

49

50

64

57

58

72

65

66

80

73

74

0.6485

88

81

82

89
8

28

21

22

15

27

36

29

30

23

35

44

37

38

31

43

52

45

46

39

51

C

59

60

53

54

47

68

61

62

55

67

76

69

70

63

75
5

84

77
7

78

71

83

86

79

B

3

0.3024

92

93

4

11

12

5

0.2082

94

6

19

20

13

14

27
2

28

21

22

E

35

36

29

30

43

44

37

38

95

7

0.0986

15

23

31

52

46

39

51

45

47

59

60

53

54

0.1433
87

0.4660
0.3349

91

85

A

90

0.4394

12

6

17

11

5

24

C

67

68

61

62

75

76

69

70

B

83

84

77

78

91

0.2407

92

85

86

93

0.1730

94

0.1243
55

63

71
7
1

79

87

95

0.0893

Figure 3. Distance matrix visualizations of the maps shown in Figure 1. These visualizations are
linked by position to Figure 2, meaning that the same node in the left two maps refer to the same
Voronoi region in the data space, and similarly in the right two maps.

using an attribute selection measure [8], such as information gain or gain ratio, as shown in Figures 5 and 7. As a
SOM produces a smaller but representative dataset, the prototype vectors can be used as an approximation of the whole
dataset. Efficient computation allows an analyst to explore
distinctive features of any region of the map interactively.

5. Results and Discussion
Our method has been tested using our Java SOM Toolbox (JSOM) on both synthetic and real-life datasets. Synthetic datasets were used to evaluate the ability of the
proposed method to visualize individual known cluster
changes, such as the introduction of new clusters and disappearing clusters. Due to space limitations only one combined scenario has been presented in Section 4. The synthetic dataset D(τ1 ) has been generated using Gaussian distributions, while the dataset D(τ2 ) has been generated using
a transition matrix P = {pij } [8], which contains the probability of an entity moving from cluster i in dataset D(τ1 )
to cluster j in dataset D(τ2 ).

5.1. World Development Indicator Data
We evaluated our method using selected indicators from
the World Development Indicator (WDI) dataset [24],
which is a multi-variate temporal dataset covering 205
countries [5]. Yearly values were grouped for 10-year periods, and the latest available values are used. The experiments compare cluster structures based on the selected indicators that reflect different aspects of welfare, such as population, life expectancy, mortality rate, immunization, illiteracy rate, education, television, and inflation.
The visualizations in Figure 4 reveal several interesting changes. First, the cluster at the bottom-left of the
1980s map is missing in the 1990s map. This cluster consists of four South American countries: Brazil, Argentina,
Nicaragua, and Peru. These countries were suffering economic difficulties (e.g. high inflation) due to a debt crisis in
the 1980s, which is known as the ‘lost decade’ [26]. However, South American countries performed rapid reforms in
the late 1980s and early 1990s [26]. The welfare of these
countries therefore became more similar to other countries,
which explains the missing cluster in the 1990s.

Relative density of map-WDI1980Training
g
0

9

1

2

18

10

11

3

4

27

19

20

12

13

36

28

29

21

22

45

30

31

46

47

39

40

63
6

shrinking
cluster

37

38

54

55

56

48

49

64
64

57

81

73

90

82

3.0000

99

91

0

100

9

1

18

10

27

19

36

28

45

37

54

46

63

55

72

64

81

73

90

82

3.0000

99

91

100

2.0000

65

58

Relative density of map-WDI1990Training
72

74

66

67

83

75

76

92

84

85

101

93

94

2.0000
2

102

1.0000

103

11

3

4

20

12

13

29

21

22

38

30

31

47

39

40

56

48

49

65

57

58

74

66

67

83

75

76

92

84

85

101

93

94

102

0.0
5

6

14

15

7

23

24

16

32

33

25

41

42

34

50

51

43

59

60

52

68

69

61

77

78

70

86

87

79

95

96

88

0.0

104

5

-1.0000

105

97

6

106

14

15

7

23

24

16

32

33

25

41

42

34

50

51

43

59

60

52

68

69

61

77

78

70

86

87

79

95

96

88

104

-1.0000

105

97

106

-2.000
8

17

26
2

35

44

53

62

71

80

89

98

107

lost cluster
0

9

1

18

10

27

19

17

26

35

44

53

cluster
enlargement

62

71

-3.000

36

28

-2.000
8

Distance matrix of map-WDI1980Training
45

37

54

46

80

89

98
8

107

-3.000

Distance matrix of map-WDI1990Training

63

55

72

64

81

73

90

82

4.0095

99

91

0

100

9

1

18

10

27

19

36

28

45

37

54

46

63

55

72

64

81

73

90

82

3.5600

99

91

100

2.9401
2

11

3

4

20

12

13

29

21

22

38

30

31

47

39

40

56

48

49

65

57

58

74

66

67

83

75

76

92

84

85

101

93

94

2.6221
2

102

2.1559

103

11

3

4

20

12

13

29

21

22

38

30

31

47

39

40

56

48

49

65

57

58

74

66

67

83

75

76

92

84

85

101

93

94

102

6

14

15

7

23

24

16
1

32

33

25

41

42

34

50

51

43

59

60

52

68

69

61

77

78

70

86

87

79

95

96

88

1.4226

104

5

1.1592

105

97

6

106

14

15

7

23

24

16

32

33

25

41

42

34

50

51

43

59

60

52

68

69

61

77

78

70

86

87

79

95

96

88

104

17

26

35

44

53

62

71

80

lost cluster

89

98

107

106

0.7718
8

0.6233

1.0478

105

97

0.8500
8

1.9314

103

1.5809
5

1.0000

103

17

26

35

44

53

62

cluster
enlargement
71

80

89

98

107

0.5685

Figure 4. The world’s welfare and poverty maps of the 1980s (left) and the 1990s (right): relative
density visualizations (top) and distance matrix-based visualizations (bottom).

Another interesting finding is that there is a cluster enlargement towards the tail of the distribution at the bottomright corner of the 1990s map. This new region consists of
OECD (Organization for Economic Co-operation and Development) and other developed countries who achieved a
higher standard of living in the 1990s, that have not been
achieved in the 1980s. However, this region cannot be considered as a new cluster, as it does not have a good separation from its left neighbours, as shown in the distance matrix visualization of the 1990s map (the bottom-right map in
Figure 4).
Finally, the top-right region of the 1980s map experienced a decrease in density compared to the 1990s map.
This region consists of several African countries. Generated
by applying hot-spot analysis [6], Figure 5 can be used to
understand distinctive features of this shrinking region. After selecting this region, the sorted component planes show
that this region is characterized by high illiteracy, high mortality rate, high percentage of children in the labor force,
low ratio of physicians, and low school enrolment.

5.2. Australian Taxation Data
Our method has been used to explore changes in cluster
structures in very large anonymized taxpayer datasets from
2003 to 2007 for the Australian Taxation Office (ATO).
Here, we provide aggregate indicative results that demon-

strate the effectiveness of our method, without breaching
the confidentiality of the data or the specifics of the discoveries made.
The datasets consist of nearly 2.8 million entities, each
with 83 numeric attributes, such as income from various
sources, work-related expenses, and tax deductions, from
2003 to 2007. The datasets were pre-processed [5] and imported into the embedded database. The map size chosen
was 15x20, and a map for each year was trained in around 6
hours on a Debian GNU/Linux machine running on a 64-bit,
2.6 GHz AMD Opteron, dual-core quad processor server,
with 32 GB main memory. During the map training, the
Java SOM Toolbox only used approximately 4 GB of memory (our JSOM Toolbox avoids loading the whole datasets
into memory).
The top map in Figure 6 shows the emergence of a new
large region (the dark blue region at the top of the map)
in the 2007 dataset, compared to the 2006 dataset. This
change is identified as a cluster enlargement as it does not
have a good separation with its neighbours, as shown in the
distance matrix visualization (the bottom map in Figure 6).
This kind of massive change in cluster structure was not
found to exist in previous time periods (from 2003 to 2006).
Noting that a SOM performs vector quantization, the size of
this region reflects the magnitude of the population affected
by the change. Thus, this is a sizable change in the behavior
of the population.

Sorted Component Plane of map map-WDI1980Training by Gain Ratio
1.00 - ILLITERACYRATEADULTFEMALE
0

9

18

1

2

10

11

20

3

4

12

14

8

38

30

24

16

17

32

42

34

56

50

51

44

65

83

75

68

69

77

71

87

79

95

80

89

107

20

14

8

29

23

17

38

32

26

47

41

35

56

50

44

65

59

53

74

68

62

83

71

92

77

86

80

89

2

102

104

98

0

9

18

1

2

10

20

3

4

12

22

14

8

30

40

32

17

26

48

49

42

34

51

44

66

67

69

107

8

70

84

71

93

87

80

89

29.777

3

15.070

6

106

98

12

15

107

8

21

24

17

30

33

26

39

17

38

30

31

23

24

16

32

26

47

41

35

56

50

44

65

59

53

74

68

62

83

77

71

92

80

86

101

102

42

35

48

51

43

44

63

57

60

53

66

69

62

75

78

71

84

95

104

9.7893

105

97

98

106

107

4.3457

87

79

80

93

96

89

102

3.1054

98

3

6

106

107

21

24

17

30

33

26

39

42

35

48

51

44

57

60

53

66

69

62

75

78

71

84

87

79

80

93

102

96

104

38.877

105

97

98

106

107

0.0618

0.75 - SCHOOLENROLLMENTSECONDARYFEMALE
0.73 - MORTALITYRATEINFANT

68.182

103

95

88

89

100

101

94

86

97.486

99

91

92

85

77

70

90

82

83

76

68

61

81

73

74

67

59

52

72

64

65

58

50

43

63

55

56

49

41

34

54

46

47

40

32

25

45

37

38

31

23

16

36

28

29

22

14

15

8

0.3631

12

7

27

19

20

13

5

1.5836

105

18

10

11

4

104

97

9

1

2

103

95

88

0

100

101

94

86

4.6273

99

91

92

85

77

70

90

82

83

76

68

61

81

73

74

67

59

52

72

64

65

58

50

15.232

103

96

88

89

100

93

94

87

79

20.676

99

91

84

85

78

70

90

82

75

76

69

61

81

73

66

67

60

52

72

64

57

58

51

43

63

55

48

49

42

34

54

46

39

40

33

25

45

37

0.75 - SCHOOLENROLLMENTSECONDARY

55

56

49

41

34

54

46

47

40

32

25

45

37

38

31

23

16

36

28

29

22

14

7

27

19

20

13

5

105

97

18

10

11

4

104

96

9

1

103

95

88

0

2

102

94

86

79

44.484

29

21

22

14

36

28

2.0626

100

101

20

15

7

27

19

12

13

6

106

0.77 - PHYSICIANS

99

91

92

85

77

78

61

62

75

68

90

82

83

76

59

60

81

73

74

57

52

53

72

64

65

58

50

43

35

63

55

56

41

33

25

54

46

47

39

31

24

16

45

37

38

23

15

7

36

28

29

21

13

5

6

27

19

11

11

5

25.765

105

97

18

10

3

4

3.0471

0.81 - LABORFORCECHILDREN

49.468

103

95

9

1

101

96

88

0

100

93

94

87

79

0.81 - DEATHRATE
73.171

99

91

84

85

78

70

90

82

75

76

69

61

81

73

66

67

60

52

72

64

57

58

51

43

63

55

48

49

42

34

54

46

39

40

33

25

45

37

30

31

24

16

36

28

21

22

15

7

27

19

12

13

6

106

98

11

5

29.899

105

97

18

10

3

4

104

96

9

2

56.752

102

103

88

0

1

101

93

94

86

78

70

1.00 - ILLITERACYRATEADULTTOTAL

100

92

85

83.605

99

91

84

76

61

90

82

74

59

62

81

73

66

67

60

52

53

72

64

57

58

43

35

63

55

48

49

41

33

26

47

40

25

54

46

39

31

23

15

7

45

37

29

22

5

36

28

21

13

6

27

19

9.5736

0.59 - SCHOOLENROLLMENTPRIMARYFEMALE

Figure 5. Top distinctive attributes for the shrinking top-right region of the 1980s map in Figure 4.
0

9

18

1

2

10

11

20

3

4

27

12

28

22

14

45

38

30

46

47

40

32

63

48

49

64

65

50

81

73

66

67

83

75

68

101

77

93

2

67.065

102

94

6
15

1

2

15
30

16

17

32

3

33

6

21

7

8

34

20

62

103

48

22

37

0

38

24

9

66

52

25

11

68

135

1 2 1 52

122

108

86

61

123

167

195

1 879
1

95

104

182

168

169

155

197

88

183

227

40

55

34 1

4

42

28

43

29

12

44

58

22

21

87

14

115

103

119

133

172

57

162

228

229

215

175

193

68

164

179

4

19

7

34

22

10

25

11

12

40

28

29

55

70

85

100

232

84

222

115

103

104

130

118

119

145

131

132

160

93

175

163

164

190

176

177

205

193

194

208

209

232

247

235

250

236

237

265

251

252

238

253

254

86

79

2

102

103.54

103

95

11

3

4

104

18

10

20

12

13

5

27

19

29

21

22

14

36

28

38

30

31

23

45

37

47

39

40

32

54

46

56

48

49

41

63

55

65

57

58

50

72

64

74

66

67

59

81

73

83

75

76

68

90

82

92

84

85

77

100

101

93

94

86

117.35

99

91

102

96

88

89

72

87.781

103

95

104

year for the financial year 2006/2007 [20]. This also explains why low values of taxable income was also a distinctive feature, as seen in Figure 7.
87

80

63

20

29

38

47

55

56

3

12

13

21

22

30

31

39

40

48

49

64

65

57

58

-3.000
6

23

24

16

12.127
8

14

15

17

32

33

25

26

41

42

34

35

50

51

43

55.881

105

97

98

81

73

74

44

59

6

106

15

7

107

90

8

24

16

17

33

25

26

42

34

35

51

43

44

60

52

53

69

61

62

78

70

71

87

79

80

96

88

89

58.207

105

97

98

106

107

0.54 - DAILYNEWSPAPERS
253.50

99

28.634

0

9

18

27

36

45

54

63

72

81

90

426.55

99

6. Conclusions and Future Work
82

83

75

76

53

62

91

92

84

100

1

101

93

2

102

172.68

10

11

3

19

20

12

28

29

21

37

38

85

94

103

4

13

22

30

31

46

47

39

40

292

294

4.1426
2.4212
1.4150

295

281

296

0.8270

297

283

298

299

77

78

70

55

56

48

64

65

57

73

74

66

82

83

75

91

92

84

100

101

49

58

67

76

85

93

94

102

285.45

103

71

80

We have introduced a relative density SOM (ReDSOM)
visualization that is able to show various changes in cluster structures, such as emerging clusters, disappearing clusters, cluster enlargements, cluster contractions, movement
of cluster centroids, and changes in cluster density. ReDSOM has been tested with real-life datasets, including large
datasets from the Australian Taxation Office.
Experiments using real-life datasets have shown that
ReDSOM is capable of indicating actual changes, such as
the change in economic fortunes of South American countries between the 1980s and 1990s, or the change in tax policy for low income earners.
These structural changes can be analyzed further by
looking into the migration patterns of the entities. Future
work incorporating migration analysis is underway.
86

87

79

95

96

88

89

104

98

5

91.864

105

97

6

106

14

15

7

107

8

23

24

16

17

32

33

25

26

41

42

34

35

0.4833

Figure 6. Relative density visualization rd2 of
the 2007 to the 2006 ATO dataset (top) and
distance matrix visualization (bottom) of the
2007 ATO dataset.

50

51

43

44

11.044

7.0881

290

68

69

61

293

282

66

67

60

52

291

280

284

70

71

54

46

288

279

268

269

45

37

4.9883

277

266

267

36

28

289

278

264

27

19

286

275

276

262

18

10

11

4

287

274

263

249

61

62

77

78

101

93

94

9

1

-2.000

285

273

260

261

248

234

239

259

245

246

9

-1.0000

1.999K

271

272

258

68

69

92

84

85

0

100

298

299

270

256

257

244

233

223

224

0

7

243

230

221

0.0

296

297

283

284

255

52

53

83

151.20

99

91

8.2181

5

241

242

43

44

59

60

90

82

75

76

295

1.002K

240

228

219

222

294

281

282

269

107

226

231

220

280

268

34

35

50

51

74

66

67

1.0000

106

225

217

206

207

292

104

229

218

204

191

192

178

179

202

203

189

253

41

42

65

57

58

81

73

293

279

266

267

25

56

48

49

72

64

0.57 - MORTALITYRATEUNDER5
2.996K

277

105

215

216

265

32

33

26

47

39

40

63

55

5.8048

290

1

102
251

254

16

17

38

54

46

288

291

278

264

23

45

37

30

31

24

2.0000

2

252

227

214

262

14

7

29

21

22

15

36

28

289

275

276

263

250

97

213

200

201

247

103

238

211

198

187

98

273

274

260

249

236

237

239

212

199

188

174

210

6

100

96

196

185

186

172

161

162

148

149

184

173

159

89

197

183

170

171

157

146

147

133

134

169

158

144

195

181

182

168

155

156

142

180

166

167

154

143

80

165

153

140

141

129

116

117

138

127

71

151

99

95

224

88

258

261

248

235

223

87

79

287

20

3.0000

286

101

221

94

272

259

245

234

86
209

78

70

152

139

128

114

150

136

125

126

112

101

102

88

89

124

113

99

62

137

123

110

111

97

86

87

73

74

109

98

84

135

121

122

108

95

96

82

71

72

58

59

94

83

69

56

57

43

44

67

120

106

107

93

80

81

53

105

91

92

79

68

44

90

78

65

66

54

41

42

63

52

35

76

77

64

53

39

75

61

50

51

37

26

27

13

49

38

24

26

62

48

35

36

23

9

60

46

47

33

20

21

8

45

31

32

18

5

6

17

30

16

17

3

14

8

15

1

2

243

91

92

208

77
194

69

61

85

285

271

27

19

12

13

8

246

233

219

220

206

207

76

178

290
17

218

205

75

191

192

231

Distance matrix of map-etax_trdb-2007Norm
0

257

244

230

82
204

83

190

661 7 6

177

163

60

812 0 2

203

189

74

161

67

187

216

73

174

59

149

52

72

201

188

64

65

148

51

43

186

173

159

160

146

58

50
134

42

34

48

147

49

118

1 5 763

55

145

131

132

171

158

144

56

41

104

33

142
54

143

130

39

116

117

156

46
129

47

40

32

89

127

128

114

30 1 0 1

102

25

45

141

37

100

88

24

16

112

38

86

31

126

113

99

23
74

15

7

36

28

29

73

59

6

97

98

84

85

71

72

13

111

8 227

83

70

56

57

96

270

256

107

213

214

200

255

241
106

242

98

198

199

185

97

240

226

11

5

36.434

105
225

211

212

89

184

170

96
210

196

80

153

154

140

87

180

701 6 6

71

138

139

125

78
165

151

152

62

124

110

69
150

136

137

53

109

95

19
69

20

5
14

93

94

81

10
54

11

26

27

13

107

80

60

120

143
06

44

78

186 7

53

39

2

12

92

79

65

51

105

34 9 1

35

63

51

1

10

77

64

50

42
90

76

18

10

3

4

0.58 - INFLATIONFOODPRICES
36

23

9

25

26

49

35

33
75

61

17

18

19

5

60

4 6 16

47

8

4

24

45

73 1

9

1

Relative density of map-etax_trdb-2007Norm
0

0

100

92

85

97.695

99

91

84

76

59

90

82

74

57

58

41

72

55

56

39

31

23

54

37

29

21

13

5

36

19

59

60

52

53

68

69

61

62

77

78

70

71

86

87

79

80

95

96

88

89

104

98

144.34

105

97

106

107

3.2461

Acknowledgement
Further analysis of the discriminating characteristics of
this new region lead to insights that are important to the
taxation analysts. The top distinctive feature was found to
relate to low income rebate amounts, as shown in Figure 7.
It was noted, in comparing the values to the 2006 map, that
the maximum value of the low income rebate amount had
doubled. Without any other knowledge, a change in behavior was identified through the deployment of ReDSOM.
An investigation, conducted after discovering this behavioral change, found that it was caused by a change in
government policy. In 2006, the Australian Government increased the Low Income Tax Offset from $235 to $600 per

This research has been supported by the Australian Taxation Office. The authors express their gratitude to Elea Gudgeon for providing data and domain expertise, to AusAID
for providing a PhD scholarship to the first author, and to
the reviewers for providing useful feedbacks. Map colors
are based on www.ColorBrewer.org.

References
[1] G. Adomavicius and J. Bockstedt. C-trend: Temporal cluster graphs for identifying and visualizing trends in multi-

0.92 - LOW_INCM_RBT_AMT

0.44 - TXBL_INCM_AMT

593.88

0.47 - NET_TAX_AMT

55.54K

0.44 - GROSS_TAX_AMT

55.56K

396.10

37.05K

37.07K

198.32

18.56K

18.58K

0.5452

72.576

91.886

163.0K

0.37 - TOTL_INCM_LOSS_AMT

175.6K

0.32 - TOTL_TAX_WITHHELD_AMT

49.98K

109.7K

118.0K

33.43K

56.35K

60.49K

16.87K

2.983K

2.917K

327.33

Figure 7. Top distinctive features for the selected region in Figure 6.

[2]
[3]

[4]
[5]

[6]

[7]

[8]

[9]

[10]

[11]

[12]
[13]

attribute transactional data. IEEE TKDE, 20(6):721–735,
2008.
C. Aggarwal, J. Han, J. Wang, and P. Yu. A framework for
clustering evolving data streams. VLDB, 29:81–92, 2003.
D. Chakrabarti, R. Kumar, and A. Tomkins. Evolutionary
clustering. In ACM SIGKDD 2006, pages 554–560, New
York, NY, USA, 2006.
G. Deboeck and T. Kohonen. Visual Explorations in Finance
with Self-Organizing Maps. Springer-Verlag, London, 1998.
Denny and D. M. Squire. Visualization of cluster changes by
comparing Self-Organizing Maps. In PAKDD 2005, volume
3518 of LNCS, pages 410–419. Springer, 2005.
Denny, G. J. Williams, and P. Christen. Exploratory hot
spot profile analysis using interactive visual drill-down selforganizing maps. In PAKDD 2008, volume 5012 of LNCS,
pages 536–543. Springer, 2008.
V. Ganti, J. Gehrke, R. Ramakrishnan, and W.-Y. Loh. A
framework for measuring differences in data characteristics. Journal of Computer and System Sciences, 64:542–578,
May 2002.
J. Han and M. Kamber. Data Mining: Concepts and Techniques (second edition). Morgan Kaufmann, San Francisco,
CA, 2006.
S. Hido, T. Idé, H. Kashima, H. Kubo, and H. Matsuzawa.
Unsupervised change analysis using supervised learning.
In PAKDD 2008, volume 5012 of LNCS, pages 148–159.
Springer, 2008.
J. Iivarinen, T. Kohonen, J. Kangas, and S. Kaski. Visualizing the clusters on the Self-Organizing Map. In Conference on AI Research in Finland, volume 12, pages 122–126.
Finnish AI Society, 1994.
S. Kaski and K. Lagus. Comparing Self-Organizing Maps.
In ICANN’96, Bochum, Germany, volume 1112 of LNCS,
pages 809–814. Springer, Berlin, 1996.
D. A. Keim. Information visualization and visual data mining. IEEE Trans. Vis. Comput. Graph., 8(1):1–8, 2002.
E. Keogh, J. Lin, and W. Truppel. Clustering of time series
subsequences is meaningless: Implications for previous and

[14]
[15]

[16]

[17]

[18]

[19]
[20]
[21]

[22]
[23]

[24]
[25]

[26]

future research. In IEEE ICDM 2003, page 115, Washington, DC, USA, 2003.
T. Kohonen. Self-organized formation of topologically correct feature maps. Biological Cybernetics, 43:59–69, 1982.
T. Kohonen.
Self-Organizing Maps (Third Edition),
volume 30 of Springer Series in Information Sciences.
Springer, Berlin, Heidelberg, 2001.
P. Lingras, M. Hogo, M. Snorek, and C. West. Temporal
analysis of clusters of supermarket customers: conventional
vs. interval set approach. Inf. Sci., 172(1-2):215–240, 2005.
H. Ritter, T. Martinetz, and K. Schulten. Neural Computation and Self-Organizing Maps; An Introduction. AddisonWesley Longman Publishing, Boston, USA, 1992.
J. F. Roddick and M. Spiliopoulou. A survey of temporal
knowledge discovery paradigms and methods. IEEE TKDE,
14(4):750–767, 2002.
J. C. Schlimmer and R. H. Granger. Incremental learning
from noisy data. Machine Learning, 1(3):317–354, 1986.
The Treasury - Australian Government. Press release no.
066, July 2006. http://www.treasurer.gov.au/.
V. Tryba, S. Metzen, and K. Goser. Designing basic integrated circuits by Self-Organizing Feature Maps. In NeuroNı̂mes’89. Intl. Workshop on Neural Networks and their
Applications, pages 225–235, Nanterre, France, November
1989. ARC; SEE, EC2.
J. Vesanto and E. Alhoniemi. Clustering of the SelfOrganizing Map. IEEE TNN, 11(3):586–600, May 2000.
G. Widmer and M. Kubat. Learning in the presence of concept drift and hidden contexts. Machine Learning, 23(1):69–
101, 1996.
World Bank. World Development Indicators 2003. The
World Bank, Washington DC, 2003.
K. Yamanishi, J.-I. Takeuchi, G. Williams, and P. Milne. Online unsupervised outlier detection using finite mixtures with
discounting learning algorithms. Data Mining and Knowledge Discovery, 8(3):275–300, 2004.
R. Zagha and G. T. Nankani, editors. Economic Growth in
the 1990s: Learning from a Decade of Reform. World Bank
Publications, Washington, DC, 2005.

