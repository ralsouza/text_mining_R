ReDSOM: Relative Density Visualization of Temporal Changes in Cluster Structures using Self-Organizing Maps
Denny Department of Computer Science, The Australian National University, Canberra, Australia
Faculty of Computer Science, University of Indonesia, Indonesia denny@cs.anu.edu.au, denny@cs.ui.ac.id
Graham J. Williams Australian Taxation Office, Canberra, Australia
graham.williams@ato.gov.au
Peter Christen Department of Computer Science, The Australian National University, Canberra, Australia
peter.christen@anu.edu.au

Abstract
We introduce a Self-Organizing Map (SOM) based visualization method that compares cluster structures in temporal datasets using Relative Density SOM (ReDSOM) visualization. Our method, combined with a distance matrix-based visualization, is capable of visually identifying emerging clusters, disappearing clusters, enlarging clusters, contracting clusters, the shifting of cluster centroids, and changes in cluster density. For example, when a region in a SOM becomes significantly more dense compared to an earlier SOM, and well separated from other regions, then the new region can be said to represent a new cluster. The capabilities of ReDSOM are demonstrated using synthetic datasets, as well as real-life datasets from the World Bank and the Australian Taxation Office. The results on the real-life datasets demonstrate that changes identified interactively can be related to actual changes. The identification of such cluster changes is important in many contexts, including the exploration of changes in population behavior in the context of compliance and fraud in taxation.
1. Introduction
Businesses and government organizations need knowledge of change in order to adapt their strategies to everchanging environments. Knowing what has changed can be a major competitive advantage for an organization. To un-

derstand what has changed, analysts have to be able to relate new knowledge or models acquired from a newer dataset to those acquired from an earlier dataset. Without this context, it can be difficult to revise existing strategies. This is particularly problematic if an organization has already implemented a strategy based on an earlier model.
In supervised learning, classifier performance often degrades over time, an issue known as concept drift [19, 23]. In many real-life domains, a concept of interest may depend on some hidden context, which is not given explicitly in the form of predictive features (i.e. some variables are invisible to the learner). For example, such hidden concepts can be changes in economic policy, disasters, life events, or changes in marketing strategies. Changes in the hidden context can induce more or less radical changes in a target concept. Most research in concept drift only addresses concept drift in a supervised learning context--little has been researched in the context of unsupervised learning.
In data mining of conceptual changes, a number of temporal data mining algorithms have focused on detecting the point in time when something has changed (change detection), rather than understanding or exploring the causes that have made the changes (change analysis). For example, by gradually eliminating the effects of past data, an on-line discounting learning algorithm can detect outliers and the change points in time in a changing data source [25].
To discover changes between two datasets, the resulting data mining models can be compared, given that a data mining model is designed to capture specific characteristics of a dataset. A theoretical framework has been introduced in [7] that allows measuring changes between two models. In this

framework, when the structural components of the models are different, both structures are `extended' to a greatest common refinement. The deviation between two models is then calculated by aggregating the differences in the measurement components of the models.
This paper focuses on visualizing, identifying, and analyzing changes in cluster structures in a SOM, trained with a newer dataset, compared to a SOM trained with an older dataset. Temporal cluster analysis can be useful to understand changes in datasets, or to review the effectiveness of deployed strategies. For example, if an organization has devised a marketing strategy based on a clustering of the past year's customer data, it is important to know if the current year's clustering differs, in order to understand changes in customer behavior, or to review the effectiveness of the implemented marketing strategy. New strategies can then be devised to encourage or deter the development of new clusters or to slow the demise of clusters, as suits the requirements of the business.
ReDSOM visualization allows users to explore the distinctive features of changes interactively using the hot-spot methodology [6]. Involving the user in the data exploration process is important in ensuring effective data analysis [12].
We propose the use of SOMs for effectively visualizing changes. SOMs have several advantages in temporal cluster analysis. They are able to relate clustering results by linking multiple visualizations, and they can detect various types of cluster changes, including emerging and disappearing clusters [5]. Furthermore, SOMs create a smaller but representative dataset, and they have topology preservation properties [15]. Importantly, SOMs can be used to explore high-dimensional data spaces through a non-linear projection onto a two-dimensional (2-D) plane using visualizations that are easy to understand even by non-analysts [15]. A mathematical analysis of SOM properties can be found in [17]. Applications of SOM for data mining are found in engineering, speech analysis and recognition, finance, and information retrieval [4, 15].
We contrast our ReDSOM visualization methodology to the goals of time series clustering [13]. Time-series clustering aims to cluster entities that have similar time-series patterns, whereas our method clusters entities at points in time (snapshots), and compares the clustering structures of such snapshots.
Research in data stream mining has also provided some insights into the problem described here. Aggarwal et.al. [2] presented a framework for clustering data streams where the aim is to discover changes in the evolving data streams. The basic idea of the approach is to divide clustering into an online and off-line component, with the online component periodically storing summary information of the data stream clusters (so called micro-clusters), and the off-line component generating aggregated clusters according to the needs

of an analyst. Our work does not consider data streams but investigates snapshot datasets that were collected at different points in time. While data streams are becoming common in many application areas, static snapshot datasets are still the most commonly used type of data in many organizations. There still remains a need to analyze changes between such snapshots.
The main contribution of this paper is the development of Relative Density SOM (ReDSOM) visualization that can compare and contrast changes in cluster structures in temporal datasets. ReDSOM allows analysts to explore and understand changes interactively. We evaluate our method on synthetic datasets and on real-life datasets published by the World Bank [24]. We have also evaluated our method on large real-life datasets from the Australian Taxation Office. The results on the real-life datasets demonstrate that changes identified interactively can be related to actual reallife changes.
The remainder of the paper is organized as follows. The next section discusses related work in temporal cluster analysis. An overview of Self-Organizing Maps is provided in Section 3. Section 4 introduces our relative density definition and ReDSOM visualization. The results of our experiments are then discussed in Section 5, and conclusions and future work are provided in Section 6.
2. Related Work
Temporal data can be grouped into four broad categories: static, sequences, time-stamped, and fully temporal [18]. In static datasets, temporal context is not included and cannot be inferred. Sequences are basically ordered lists of events, but not time-stamped. Examples of time-stamped datasets are census data, web-based activity, or sales transaction. In fully temporal databases, each tuple in a time-varying relation in the database may have one or more dimensions of time, such as age and treatment time. Our method analyzes changes in two or more static temporal datasets.
Chakrabarti, Kumar, and Tomkins [3] defined evolutionary clustering as the problem of processing time-stamped data to produce a sequence of clusterings; that is, a clustering for each time step. This framework tries to optimize two potentially conflicting criteria: remaining faithful to the current data, and not shifting dramatically from the previous clustering results. Therefore, the user has to define a snapshot quality function sq(Ct, Dt) which measures the quality of a clustering result Ct for dataset Dt at time t, and a history cost function hc(Ct-1, Ct) which measures how much a latter clustering result Ct differs from the previous one, Ct-1. The optimal cluster sequence can, therefore, be found by determining at each time step t a clustering Ct that optimizes the incremental quality sq(Ct, Dt) - cp · hc(Ct-1, Ct), where cp is a non-negative

change parameter. As cp is increased, more weight is placed on matching the historical clusters. Based on this, the authors derived an agglomerative hierarchical and a k-means clustering algorithm. When calculating clustering result Ct using k-means, for example, the previous cluster centroids of Ct-1 are used as the starting seeds. The new centroids are then calculated based on the closest match of cluster centroids in the previous clustering result, and the cluster centroids of the non-evolutionary (conventional) k-means.
This framework is able to find a balance between remaining faithful and not shifting dramatically in training the subsequent clustering results, in order to smooth the clustering sequence. However, the aim of this framework is not to analyze the changes of the clustering results. It is not easy to understand the actual cluster changes using plots of snapshot quality and historical cost over time. It is not clear how to relate and understand changes in terms of the earlier clustering results. Furthermore, the capability of the framework to detect any rate of changes (abrupt or gradual changes) is questioned as the change parameter cp is constant over time and has to be defined beforehand.
The k-means version of the framework also has some issues related to the discovery of new or lost clusters. With the proposed k-means variant, the previous cluster centroids Ct-1 are used as starting seeds. A problem arises when there is a larger number of cluster (k) selected in finding the latter clustering result Ct. It is not clear how to initialize the additional cluster centroids in this case. A similar problem occurs when a smaller k is selected. In this case, one or more cluster centroids from the previous clustering result Ct-1 have to be removed in finding the latter clustering result Ct. Our method, on the other hand, is able to show emerging clusters and lost clusters without having to determine the number of clusters beforehand.
Hido et.al. [9] proposed an approach to explain changes between two datasets by using a decision tree, and labeling one dataset as negative and the other as positive. The trained model is then investigated to understand differences between the datasets. As decision trees divide the data space into hypercubes and try to separate the entities based on their labels, this approach can detect when there is a new hypercube that was more sparsely occupied by the other dataset. However, this method cannot show the separation between the hypercubes and the density of these hypercubes. Therefore, this method cannot differentiate between emerging clusters and cluster enlargements. In our method, on the other hand, separation between prototype vectors can be shown using distance matrix visualization [10]. Furthermore, when correlated attributes exist in the dataset, only one of them will be used to describe new or lost hypercubes, reducing the description of the hypercubes.
Recently, Adomavicius and Bockstedt [1] introduced a graph visualization technique for exploring trends in multi-

attribute temporal datasets using a temporal cluster graph. In this technique, transactional dataset D is partitioned into data subsets Dt according to time periods. Each data subset is then clustered. Only clusters that have at least |Dt| number of entities are shown as nodes, where   [0, 1] is a node filter parameter. Nodes between two adjacent time periods are connected with an edge, if the distance between the nodes is less than a threshold , which is calculated based on an edge filter parameter  and the average between-cluster distances. This graph is visualized interactively, where users have to experiment with the number of clusters for each partition Dt, node filter parameter , and edge filter parameter . In our method, users do not have to experiment to find the optimal number of clusters.
For detecting changes between two SOMs, Kaski and Lagus [11] proposed a dissimilarity measure for two maps, which is calculated based on the expected value of distances between pairs of representative data points on both maps. This approach can determine how much two SOMs differ, but it cannot analyze the changes.
Lingras et.al. [16] studied the temporal cluster characteristics of supermarket customers using an interval set basedSOM. The capability of the proposed method to detect new clusters is questioned as the number of clusters was constant for all time periods in their experiments.
Denny and Squire [5] proposed a SOM training method and SOM-based visualization techniques that are capable of explaining the clustering results of a second dataset in terms of the clustering result of a first dataset by using color and position linking. These visualization techniques can relate two clustering results which can show the following structural changes: changes in cluster size, centroid movements, new clusters, cluster splitting, missing clusters, and cluster merging. Such changes have to be analyzed visually by selecting a number of clusters for both datasets. However, these techniques cannot differentiate between cluster enlargement (occupying new space), and increase in cluster density (more entities in the same space).
3. Self-Organizing Maps
A SOM is an artificial neural network that performs unsupervised competitive learning [14]. Artificial neurons are arranged on a low-dimensional grid, commonly a 2D plane with nr rows, nc columns, and a total number of nunit = nr · nc units. Each neuron j has an d-dimensional prototype vector, mj, where d is the dimensionality of dataset D. Each neuron is connected to neighboring neurons, with distances to its neighbours being equidistant in the map space. The lattice structure can be a hexagonal grid, where each neuron is connected to six neighbours. Larger maps generally have higher accuracy and generalization capability [22], but they also have higher computation costs.

Before training a map, the prototype vectors should be initialized using random initial values, or ordered values (linear initialization) [15]. Linear initialization uses ordered values of component vectors based on the first two largest principal components. When using random initialization, the radius of the neighbourhood function should be large enough; otherwise the map will not be globally ordered. However, if linear initialization is used for the initial map, then a smaller radius and a shorter training length could be used [15]. Therefore, linear initialization is preferred over random initialization, because it can speed up the learning process by orders of magnitude [15].
At each training step t, the best matching unit bi (BMU) for training data vector xi, i.e. the prototype vector mj closest to the training data vector xi, is selected from the map according to Equation 1:

j, xi - mbi (t)  xi - mj(t)

(1)

In the batch training algorithm [15], the values of new
prototype vectors mj(t + 1) are weighted averages of the training data vectors xi, where the weight is the neighbourhood kernel value hbij centered on the best matching unit bi. Since the neighbourhood function hbi,j value is the same for all data vectors mapped to the same unit, the sum SVj of the Voronoi set Vj of each prototype vector mj at training step t. So, each training data vector belonging to the
Voronoi set of its closest prototype vector (BMU), can be
calculated first using Equation 2:

SVj (t) =

xi.

xi Vj

(2)

Then, the prototype vectors are updated with:

mj(t + 1) =

nunit i=1

hji(t)

·

nunit i=1

hji(t)

SVi (t) · nVi

,

(3)

where hji is the neighbourhood kernel function centered on unit j (commonly Gaussian) [15], and nVj is the number of training data vectors in Voronoi set Vj. To handle missing values, SVj and nVj only perform summation and counting of non-missing components, respectively. This calculation of new prototype vectors takes into account neighboring prototype vectors which preserve the topological order.
The map is usually trained in two phases: a rough training phase and a fine tuning phase. The rough training phase usually has shorter training length and larger initial radius compared to fine tuning phase [15].
SOMs are popularly used in cluster analysis because they perform vector quantization and preserve topological order. Furthermore, the trained SOMs can be visualized using various methods that allow non-technical users to explore a dataset. Component plane visualizations can be used to

show the spread of values of a certain component of all prototype vectors in a SOM [21]. Distance-matrix based visualizations, such as u-matrix visualization [10], show distances between neighboring nodes using a color scale representation on a map grid. This visualization can be used to identify borders between clusters, where long distances show highly dissimilar features between neighboring nodes that divide clusters, i.e. the dense parts of a map with similar features [10].
In [22], the prototype vectors of a trained SOM can be treated as `proto-clusters' serving as an abstraction of the dataset. The prototype vectors are then clustered using a traditional clustering technique, such as k-means, to form the final clusters. In this two-level clustering, adding an extra layer simplifies the clustering task and reduces noise, but may yield higher distortion [22].
4. Relative Density
Let D(1) be a dataset at time 1, and D(2) be a dataset at time 2, where 1 < 2. In order to be able to compare two maps that are trained using datasets D(1) and D(2), the orientation of map M (1) and M (2) must be the same. Therefore, the following map training procedure, as proposed in [5], is used.
1. Normalize both datasets D(1) and D(2) using the same normalization method (e.g. z-score) and parameters (e.g. the same mean and standard deviation values) for the same attributes.
2. Initialize map M (1) using ordered values.
3. Train map M (1) using dataset D(1).
4. Initialize map M (2) using the prototype vectors of the trained map M (1).
5. Train map M (2) using dataset D(2).
4.1. Relative Density Definition
As a SOM follows the distribution of a dataset it is trained on, more prototype vectors are allocated for dense regions, as shown in Figure 1. Therefore, area density at mj(1) on its own map, M (1), might be different compared to area density at the same location on map M (2). When the area density at the location of the prototype vector mj(1) in D(2) is more sparse, the area density on map M (2) is lower, compared to the area density at the same location on map M (1), and vice-versa. In Figure 1, the area density at the marked area (the center of the Gaussian kernel contour) on the left plot is higher, compared to the area density at the same location on the right plot.

Y
0.2 0.4 0.8 0.2 0.1

2.5

2

1.5 0.1 1B

0.5 0 0.6
-0.5 A

0.6 0.4 0.2

-1

-1.5

0.2

0.1
-2 -2

-1

0.4

0.1

0.1 0.2

C
D
01 X data vector

2.5 2C

1.5 0.1
B
1

Y
0.2 0.4

0.1
0.2 0.4 0.8 0.6 0.4
0.2

E 0.5
0 0.6
A
-0.5

0.2 0.1

-1

-1.5 0.2

D

0.1

-2 0.1 2 -2 -1 0 1
X

prototype vector

Gaussian kernel function

E
2

Figure 1. Plots of data vectors and prototype vectors of the maps trained using two synthetic datasets D(1) (left) and D(2) (right), where there is an emerging cluster (`E'), a lost cluster (`A'), a more dense cluster (`B'), a less dense cluster (`D'), and an unchanged cluster (`C'), in the dataset D(2). The contour of the Gaussian kernel function centered on one of the cluster `A' prototype vectors is shown on both plots.

We define area density M()(v) at the location of a vector v on map M () as the weighted sum of similar prototype vectors mj() on M () centered on vector v, where the weight is calculated based on a Gaussian kernel function centered on vector v, as shown in Equation 4:

M()(v) =

exp

- v - mj() 2·r

j=1,...,nunit

(4)

As the radius r can be different for different maps (e.g. datasets with large attribute value ranges will need to have a larger radius), the radius should be determined based on "between neighbour distances" on the map. To adapt the radius for different maps, the quartile (e.g. third quartile) of these distances is used as the radius. As the area density is normalized into a relative density (Equation 5), this relative density is not sensitive to the radius. However, the radius should not be too small or too large. If the radius is too small, the relative density will be too sensitive to noise. On the other hand, if the radius is too large, then relative density cannot capture the details of changes.
We define relative density RDM(2)/M(1)(v) as the ratio of the area density at location of vector v on map M (2) to the area density at the same location on the reference map M (1), as shown in Equation 5:

RDM(2)/M(1)(v) = log2

M (2 ) (v) M (1 ) (v)

(5)

Without using a logarithm function in Equation 5, values between 0 and 1 are interpreted as becoming more sparse, the value of 1 is interpreted as no change, and values above 1 are interpreted as becoming more dense. Therefore, a base two logarithm is used to make it easier to interpret the ratio where positive values are interpreted as becoming more dense, negative values as becoming more sparse, and 0 is interpreted as no change. For example, a value of +2 is interpreted as the area centered at the location of vector v in the dataset D(2) being four times more dense compared to the same area in the reference dataset D(1).
Based on our observations, when the value of RDM(2)/M(1)(v) is less than -3, then the space at location of vector v is no longer occupied in the next map M (2) (it is lost). Similarly, when RDM(2)/M(1)(v) is greater than +3, then the space at location of vector v was not occupied on the reference map M (1). In other words, the space is only occupied on map M (2).
Both Equations 4 and 5 are performed on all the prototype vectors of both maps, but not on the actual data vectors. Therefore, the running time of the calculation for a map is quadratic in the number of map units nu, not in the number of data vectors nD(), where nu nD().

4.2. Relative Density Visualization
As a shorthand, let rd1  RDM(2)/M(1)(mj (1)) and rd2  RDM(2)/M(1)(mj (2)).
To visualize the relative density of the locations of all prototype vectors mj(1), the values of rd1 are visualized on a map M (1) in a gradation of blue for positive values and in a gradation of red for negative values1, as shown in the left map in Figure 2. Values over +3 are represented as dark blue, and values under -3 are represented as dark red. A value of 0 is represented as white, as it indicates no change in the density. For example, visualizations of the datasets and prototype vectors from Figure 1 can be seen in Figure 2.
However, rd1 rarely returns values greater than +3, because all prototype vectors mj(1) are always present on the reference map M (1) (prototype vector mj(1) is part of map M (1)). Therefore, M(1)(mj(1)) at least returns 1, which makes the denomination part of Equation 5 larger compared to M(1)(mj(2)). Therefore, values of rd2 should be visualized on map M (2) to detect emerging clusters, as shown in the right map in Figure 2. To detect new clusters, rd2 is used because the area at the location of prototype vector mj(2) might be empty on map M (1). Similarly, rd2 cannot be used to detect lost clusters on map M (2), because the empty space at the location of the prototype vector mj(1) is not represented on map M (2), as M (2) follows the distribution of dataset D(2).
Because of SOM's vector quantization property and our definition of relative density, prototype vectors on map M (1) that have negative rd1 values will be less represented on map M (2). For example, there are less prototype vectors in cluster `D' in the right map in Figure 2. On the other hand, prototype vectors on map M (1) that have positive rd1 values will be more represented on map M (2), as shown in cluster `B' in the right map in Figure 2.
4.3. Analysis of Changed Regions
There are several types of possible structural changes between two datasets: new clusters, lost clusters, cluster enlargements, cluster contractions, shifting of centroids, and changes in cluster density. All these structural changes can be identified using ReDSOM and distance matrix visualizations. As mentioned before, distance-matrix based visualizations show distances between neighboring nodes using a color scale representation on the map, which can be used to identify cluster borders [10]. For example, in Figure 3, there are four clusters in both datasets (light yellow regions) separated by long distances.
1The SOM visualizations presented in this paper unfortunately require a diverging color scheme to illustrate the relative density that is hard to distinguish between positive and negative values using gray scale. We hope the reader has access to the color version, at least can view the PDF file.

Identifying new clusters. New clusters in dataset D(2) can be identified by dark blue regions (values above +3) on relative density visualization rd2 of map M (2), and they have long distances at the border of the regions on map M (2), for example cluster `E', as shown in the right maps in Figures 2 and 3. When a new cluster appears inside the distribution of dataset D(1), in other words between disjoint clusters, the values of rd1 are close to +3, and the cluster is positioned in a sparse area (long distances in the distance matrix visualization between the clusters). This is due to interpolative units, which appear when the data clusters are disjoint [22], as can be seen in both plots in Figure 1. Therefore, the area density at this gap is higher, compared to the area density at the empty space outside the data distribution.
Identifying cluster enlargements, cluster contractions, movement of cluster centroids. When a new dense area emerges in dataset D(2), but it does not have a good separation to its neighbour, the changes can be interpreted as cluster enlargements. This can be identified by dark blue regions on the relative density visualization rd2, and the region has short distances at the border of the regions on map M (2), as shown in the bottom-right corner of the right maps in Figure 4. When this kind of new region appears at the border of map M (2), it can be said that the changes move towards the tail of the data distribution. Similarly, cluster contraction can be identified by a lost region, but it does not have a good separation to its neighbours. Movement of cluster centroids can be identified by simultaneous cluster enlargements and cluster contractions.
Identifying lost clusters. Lost clusters in dataset D(1) can be identified by dark red regions (value below -3) on the relative density visualization rd1 of map M (1), and long distances at the border of the regions on map M (1). An example is cluster `A' in the left maps in Figures 2 and 3.
Identifying change of cluster density. An increase of cluster density in dataset D(2) can be identified in the relative density visualization as light blue, for example cluster `B' as shown in the left map in Figure 2. On the other hand, a decrease of cluster density in dataset D(2) can be identified in the relative density visualization as light red, for example cluster `D' as shown in the left map in Figure 2.
Analyzing interesting changes. Once a region of interest is selected interactively by a user, our hot spot methodology [6] can be used to understand distinctive features of these changing regions. In this methodology, the component planes are sorted by the importance of the attributes that distinguish the region from the rest of the population

Relative density of map-lostNewCluster1
0 8 16 24 32 40 48 56 64 72 80 88
A1 9 17 25 33 41 49 57 65 73 81 89 D2 10 18 26 34 42 50 58 66 74 82 90
3 11 19 27 35 43 51 59 67 75 83 91
4 12 20 28 36 44 52 60 68 76 84 92
5 13 21 29 37 45 53 61 69 77 85 93
6 14 22 30 38 46 54 62 70 78 86 94
C B7 15 23 31 39 47 55 63 71 79 87 95

3.0000 2.0000 1.0000 0.0 -1.0000 -2.000 -3.000

Relative density of map-lostNewCluster2
0 8 16 24 32 40 48 56 64 72 80 88
D A1 9 17 25 33 41 49 57 65 73 81 89
2 10 18 26 34 42 50 58 66 74 82 90
3 11 19 27 35 43 51 59 67 75 83 91
B4 12 20 28 36 44 52 60 68 76 84 92
5 13 21 29 37 45 53 61 69 77 85 93
E6 14 22 30 38 46 54 62 70 78 86 94 C7 15 23 31 39 47 55 63 71 79 87 95

3.0000 2.0000 1.0000 0.0 -1.0000 -2.000 -3.000

Figure 2. Relative density visualizations rd1 (left) and rd2 (right) of the datasets and maps shown in Figure 1. The bar chart inside each node shows the value of components of the prototype vectors (dark gray for component `X' and light gray for component `Y').

Distance matrix of map-lostNewCluster1
0 8 16 24 32 40 48 56 64 72 80 88
A1 9 17 25 33 41 49 57 65 73 81 89 D2 10 18 26 34 42 50 58 66 74 82 90
3 11 19 27 35 43 51 59 67 75 83 91
4 12 20 28 36 44 52 60 68 76 84 92
5 13 21 29 37 45 53 61 69 77 85 93
6 14 22 30 38 46 54 62 70 78 86 94
C B7 15 23 31 39 47 55 63 71 79 87 95

0.9276 0.6384 0.4394 0.3024 0.2082 0.1433 0.0986

Distance matrix of map-lostNewCluster2
0 8 16 24 32 40 48 56 64 72 80 88
D A1 9 17 25 33 41 49 57 65 73 81 89
2 10 18 26 34 42 50 58 66 74 82 90
3 11 19 27 35 43 51 59 67 75 83 91
B4 12 20 28 36 44 52 60 68 76 84 92
5 13 21 29 37 45 53 61 69 77 85 93
E
6 14 22 30 38 46 54 62 70 78 86 94
C7 15 23 31 39 47 55 63 71 79 87 95

0.6485 0.4660 0.3349 0.2407 0.1730 0.1243 0.0893

Figure 3. Distance matrix visualizations of the maps shown in Figure 1. These visualizations are linked by position to Figure 2, meaning that the same node in the left two maps refer to the same Voronoi region in the data space, and similarly in the right two maps.

using an attribute selection measure [8], such as information gain or gain ratio, as shown in Figures 5 and 7. As a SOM produces a smaller but representative dataset, the prototype vectors can be used as an approximation of the whole dataset. Efficient computation allows an analyst to explore distinctive features of any region of the map interactively.
5. Results and Discussion
Our method has been tested using our Java SOM Toolbox (JSOM) on both synthetic and real-life datasets. Synthetic datasets were used to evaluate the ability of the proposed method to visualize individual known cluster changes, such as the introduction of new clusters and disappearing clusters. Due to space limitations only one combined scenario has been presented in Section 4. The synthetic dataset D(1) has been generated using Gaussian distributions, while the dataset D(2) has been generated using a transition matrix P = {pij} [8], which contains the probability of an entity moving from cluster i in dataset D(1) to cluster j in dataset D(2).

5.1. World Development Indicator Data
We evaluated our method using selected indicators from the World Development Indicator (WDI) dataset [24], which is a multi-variate temporal dataset covering 205 countries [5]. Yearly values were grouped for 10-year periods, and the latest available values are used. The experiments compare cluster structures based on the selected indicators that reflect different aspects of welfare, such as population, life expectancy, mortality rate, immunization, illiteracy rate, education, television, and inflation.
The visualizations in Figure 4 reveal several interesting changes. First, the cluster at the bottom-left of the 1980s map is missing in the 1990s map. This cluster consists of four South American countries: Brazil, Argentina, Nicaragua, and Peru. These countries were suffering economic difficulties (e.g. high inflation) due to a debt crisis in the 1980s, which is known as the `lost decade' [26]. However, South American countries performed rapid reforms in the late 1980s and early 1990s [26]. The welfare of these countries therefore became more similar to other countries, which explains the missing cluster in the 1990s.

Relative density of map-WDI1980Training
0 9 18 27 36 45 54 63 72 81 90 99
shrinking
1 10 19 28 37 46 55 64 73 82 91 100
cluster
2 11 20 29 38 47 56 65 74 83 92 101
3 12 21 30 39 48 57 66 75 84 93 102
4 13 22 31 40 49 58 67 76 85 94 103
5 14 23 32 41 50 59 68 77 86 95 104
6 15 24 33 42 51 60 69 78 87 96 105
7 16 25 34 43 52 61 70 79 88 97 106
8 17 26 35 44 53 62 71 80 89 98 107
lost cluster
Distance matrix of map-WDI1980Training
0 9 18 27 36 45 54 63 72 81 90 99
1 10 19 28 37 46 55 64 73 82 91 100
2 11 20 29 38 47 56 65 74 83 92 101
3 12 21 30 39 48 57 66 75 84 93 102
4 13 22 31 40 49 58 67 76 85 94 103
5 14 23 32 41 50 59 68 77 86 95 104
6 15 24 33 42 51 60 69 78 87 96 105
7 16 25 34 43 52 61 70 79 88 97 106
8 17 26 35 44 53 62 71 80 89 98 107
lost cluster

3.0000 2.0000 1.0000 0.0 -1.0000 -2.000 -3.000 4.0095 2.9401 2.1559 1.5809 1.1592 0.8500 0.6233

Relative density of map-WDI1990Training
0 9 18 27 36 45 54 63 72 81 90 99
1 10 19 28 37 46 55 64 73 82 91 100
2 11 20 29 38 47 56 65 74 83 92 101
3 12 21 30 39 48 57 66 75 84 93 102
4 13 22 31 40 49 58 67 76 85 94 103
5 14 23 32 41 50 59 68 77 86 95 104
6 15 24 33 42 51 60 69 78 87 96 105
7 16 25 34 43 52 61 70 79 88 97 106
cluster8 17 26 35 44 53 62 71 80 89 98 107 enlargement
Distance matrix of map-WDI1990Training
0 9 18 27 36 45 54 63 72 81 90 99
1 10 19 28 37 46 55 64 73 82 91 100
2 11 20 29 38 47 56 65 74 83 92 101
3 12 21 30 39 48 57 66 75 84 93 102
4 13 22 31 40 49 58 67 76 85 94 103
5 14 23 32 41 50 59 68 77 86 95 104
6 15 24 33 42 51 60 69 78 87 96 105
7 16 25 34 43 52 61 70 79 88 97 106
cluster8 17 26 35 44 53 62 71 80 89 98 107 enlargement

3.0000 2.0000 1.0000 0.0 -1.0000 -2.000 -3.000 3.5600 2.6221 1.9314 1.4226 1.0478 0.7718 0.5685

Figure 4. The world's welfare and poverty maps of the 1980s (left) and the 1990s (right): relative density visualizations (top) and distance matrix-based visualizations (bottom).

Another interesting finding is that there is a cluster enlargement towards the tail of the distribution at the bottomright corner of the 1990s map. This new region consists of OECD (Organization for Economic Co-operation and Development) and other developed countries who achieved a higher standard of living in the 1990s, that have not been achieved in the 1980s. However, this region cannot be considered as a new cluster, as it does not have a good separation from its left neighbours, as shown in the distance matrix visualization of the 1990s map (the bottom-right map in Figure 4).
Finally, the top-right region of the 1980s map experienced a decrease in density compared to the 1990s map. This region consists of several African countries. Generated by applying hot-spot analysis [6], Figure 5 can be used to understand distinctive features of this shrinking region. After selecting this region, the sorted component planes show that this region is characterized by high illiteracy, high mortality rate, high percentage of children in the labor force, low ratio of physicians, and low school enrolment.
5.2. Australian Taxation Data
Our method has been used to explore changes in cluster structures in very large anonymized taxpayer datasets from 2003 to 2007 for the Australian Taxation Office (ATO). Here, we provide aggregate indicative results that demon-

strate the effectiveness of our method, without breaching the confidentiality of the data or the specifics of the discoveries made.
The datasets consist of nearly 2.8 million entities, each with 83 numeric attributes, such as income from various sources, work-related expenses, and tax deductions, from 2003 to 2007. The datasets were pre-processed [5] and imported into the embedded database. The map size chosen was 15x20, and a map for each year was trained in around 6 hours on a Debian GNU/Linux machine running on a 64-bit, 2.6 GHz AMD Opteron, dual-core quad processor server, with 32 GB main memory. During the map training, the Java SOM Toolbox only used approximately 4 GB of memory (our JSOM Toolbox avoids loading the whole datasets into memory).
The top map in Figure 6 shows the emergence of a new large region (the dark blue region at the top of the map) in the 2007 dataset, compared to the 2006 dataset. This change is identified as a cluster enlargement as it does not have a good separation with its neighbours, as shown in the distance matrix visualization (the bottom map in Figure 6). This kind of massive change in cluster structure was not found to exist in previous time periods (from 2003 to 2006). Noting that a SOM performs vector quantization, the size of this region reflects the magnitude of the population affected by the change. Thus, this is a sizable change in the behavior of the population.

Sorted Component Plane of map map-WDI1980Training by Gain Ratio

1.00 - ILLITERACYRATEADULTFEMALE

0 9 18 27 36 45 54 63 72 81 90 99

83.605

1 10 19 28 37 46 55 64 73 82 91 100

2 11 20 29 38 47 56 65 74 83 92 101 3 12 21 30 39 48 57 66 75 84 93 102

56.752

4 13 22 31 40 49 58 67 76 85 94 103

5 14 23 32 41 50 59 68 77 86 95 104 6 15 24 33 42 51 60 69 78 87 96 105

29.899

7 16 25 34 43 52 61 70 79 88 97 106

8 17 26 35 44 53 62 71 80 89 98 107

3.0471

1.00 - ILLITERACYRATEADULTTOTAL

0 9 18 27 36 45 54 63 72 81 90 99

73.171

1 10 19 28 37 46 55 64 73 82 91 100

2 11 20 29 38 47 56 65 74 83 92 101 3 12 21 30 39 48 57 66 75 84 93 102

49.468

4 13 22 31 40 49 58 67 76 85 94 103

5 14 23 32 41 50 59 68 77 86 95 104 6 15 24 33 42 51 60 69 78 87 96 105

25.765

7 16 25 34 43 52 61 70 79 88 97 106

8 17 26 35 44 53 62 71 80 89 98 107

2.0626

0.81 - DEATHRATE
0 9 18 27 36 45 54 63 72 81 90 99 1 10 19 28 37 46 55 64 73 82 91 100
2 11 20 29 38 47 56 65 74 83 92 101 3 12 21 30 39 48 57 66 75 84 93 102
4 13 22 31 40 49 58 67 76 85 94 103 5 14 23 32 41 50 59 68 77 86 95 104
6 15 24 33 42 51 60 69 78 87 96 105 7 16 25 34 43 52 61 70 79 88 97 106
8 17 26 35 44 53 62 71 80 89 98 107

20.676 15.232 9.7893 4.3457

0.81 - LABORFORCECHILDREN
0 9 18 27 36 45 54 63 72 81 90 99 1 10 19 28 37 46 55 64 73 82 91 100
2 11 20 29 38 47 56 65 74 83 92 101 3 12 21 30 39 48 57 66 75 84 93 102
4 13 22 31 40 49 58 67 76 85 94 103 5 14 23 32 41 50 59 68 77 86 95 104
6 15 24 33 42 51 60 69 78 87 96 105 7 16 25 34 43 52 61 70 79 88 97 106
8 17 26 35 44 53 62 71 80 89 98 107

44.484 29.777 15.070 0.3631

0.77 - PHYSICIANS
0 9 18 27 36 45 54 63 72 81 90 99 1 10 19 28 37 46 55 64 73 82 91 100
2 11 20 29 38 47 56 65 74 83 92 101 3 12 21 30 39 48 57 66 75 84 93 102
4 13 22 31 40 49 58 67 76 85 94 103 5 14 23 32 41 50 59 68 77 86 95 104
6 15 24 33 42 51 60 69 78 87 96 105 7 16 25 34 43 52 61 70 79 88 97 106
8 17 26 35 44 53 62 71 80 89 98 107

4.6273 3.1054 1.5836 0.0618

0.75 - SCHOOLENROLLMENTSECONDARY

0 9 18 27 36 45 54 63 72 81 90 99

97.486

1 10 19 28 37 46 55 64 73 82 91 100

2 11 20 29 38 47 56 65 74 83 92 101 3 12 21 30 39 48 57 66 75 84 93 102

68.182

4 13 22 31 40 49 58 67 76 85 94 103

5 14 23 32 41 50 59 68 77 86 95 104 6 15 24 33 42 51 60 69 78 87 96 105

38.877

7 16 25 34 43 52 61 70 79 88 97 106

8 17 26 35 44 53 62 71 80 89 98 107

9.5736

0.75 - SCHOOLENROLLMENTSECONDARYFE0M.7A3LE- MORTALITYRATEINFANT

0.59 - SCHOOLENROLLMENTPRIMARYFEMALE

Figure 5. Top distinctive attributes for the shrinking top-right region of the 1980s map in Figure 4.0 9 18 27 36 45 54 63 72 81 90 99

97.695

0 9 18 27 36 45 54 63 72 81 90 99

151.20

0 9 18 27 36 45 54 63 72 81 90 99

117.35

1 10 19 28 37 46 55 64 73 82 91 100

1 10 19 28 37 46 55 64 73 82 91 100

1 10 19 28 37 46 55 64 73 82 91 100

2 11 20 29 38 47 56 65 74 83 92 101 3 12 21 30 39 48 57 66 75 84 93 102

67.065

2 11 20 29 38 47 56 65 74 83 92 101 3 12 21 30 39 48 57 66 75 84 93 102

103.54

2 11 20 29 38 47 56 65 74 83 92 101 3 12 21 30 39 48 57 66 75 84 93 102

87.781

4 13 22 31 40 49 58 67 76 85 94 103

4 13 22 31 40 49 58 67 76 85 94 103

4 13 22 31 40 49 58 67 76 85 94 103

5 14 23 32 41 50 59 68 77 86 95 104

Relative density of map-etax_trdb-2007Norm

36.434

6 15 24 33 42 51 60 69 78 87 96 105

0 15 30 45 60 75 90 105 120 135 150 165 180 195 210 225 240 255 270 285

1

1 6 73 1

4 616

6 1 25 7 6

34 9 1

14036

1 2 1 52

1 3 6 61 1 5 1

701 6 6

1 8719

1 9 6 88 2 1 1

97 2 2 6

12 4016

256

271

286

2 17 32 47 62 77 92 107 122 137 152 167 182 197 212 227 242 257 272 287 8 17 26 35 44 53 62 71 80 89 98 107
3 18 33 48 63 78 93 108 123 138 153 168 183 198 213 228 243 258 273 288
5.8048
4 19 34 49 64 79 94 109 124 139 154 169 184 199 214 229 244 259 274 289

5 20 35 50 65 80 95 110 125 140 155 170 185 200 215 230 245 260 275 290
0.58 - INFLATIONFOODPRICES6 21 36 51 66 81 96 111 126 141 156 171 186 201 216 231 246 261 276 291

2.996K7

22 0

37 9 52

186 7

8 227

9 7 36 1 1 2 45 1 2 7

5144 2

1 5 763

1 7 2 72 1 8 7

812 0 2

29107

2 3 2 99

247

262

277

292

8 23 38 53 68 83 98 113 128 143 158 173 188 203 218 233 248 263 278 293 1 10 19 28 37 46 55 64 73 82 91 100

9 24 39 54 69 84 99 114 129 144 159 174 189 204 219 234 249 264 279 294

2 11 20 29 38 47 56 65 74 83 92 101 10 25 40 55 70 85 100 115 130 145 160 175 190 205 220 235 250 265 280 295

1.999K

11

2 6 34 1

5 612

7 1 21 8 6

30 1 0 1

13196

1 3 1 48

1 4 6 57 1 6 1

661 7 6

1 9715

2 0 6 84 2 2 1

93 2 3 6

12 5012

266

281

296

12 27 42 57 72 87 102 117 132 147 162 177 192 207 222 237 252 267 282 297 4 13 22 31 40 49 58 67 76 85 94 103

13 28 43 58 73 88 103 118 133 148 163 178 193 208 223 238 253 268 283 298 5 14 23 32 41 50 59 68 77 86 95 104

14 29 44 59 74 89 104 119 134 149 164 179 194 209 224 239 254 269 284 299

6 15 24 33 42 51 60 69 78 87 96 105

1.002K

Distance7 ma16 trix25 of m34 ap43 -eta52 x_tr61 db-270 00779 No88 rm 97

106

0

15 8 30

4157

6 0 26 7 5 35 9 0

4140 5

1 2503

1 3 5 62 1 5 0

71 1 6 5

18800

1 9 589

2 1 0 98 2 2 5

10274 0

255

270

285

4.98831 1 6 3 1 4 6 6 1 7 6 9 1 106 121 136 151 166 181 196 211 226 241 256 271 286

2 17 32 47 62 77 92 107 122 137 152 167 182 197 212 227 242 257 272 287

3 18 33 48 63 78 93 108 123 138 153 168 183 198 213 228 243 258 273 288

4 19 34 49 64 79 94 109 124 139 154 169 184 199 214 229 244 259 274 289

5 20 35 50 65 80 95 110 125 140 155 170 185 200 215 230 245 260 275 290

6 21 36 51 66 81 96 111 126 141 156 171 186 201 216 231 246 261 276 291

7 22 37 52 67 82 97 112 127 142 157 172 187 202 217 232 247 262 277 292

8 23 38 53 68 83 98 113 128 143 158 173 188 203 218 233 248 263 278 293

9 24 39 54 69 84 99 114 129 144 159 174 189 204 219 234 249 264 279 294

10 25 40 55 70 85 100 115 130 145 160 175 190 205 220 235 250 265 280 295

11 26 41 56 71 86 101 116 131 146 161 176 191 206 221 236 251 266 281 296

12 27 42 57 72 87 102 117 132 147 162 177 192 207 222 237 252 267 282 297

13 28 43 58 73 88 103 118 133 148 163 178 193 208 223 238 253 268 283 298

14 29 44 59 74 89 104 119 134 149 164 179 194 209 224 239 254 269 284 299

5 14 23 32 41 50 59 68 77 86 95 104

5 14 23 32 41 50 59 68 77 86 95 104

year for the financial year 2006/2007 [20]. This also ex-3.00006 15 24 33 42 51 60 69 78 87 96 105

55.881

6 15 24 33 42 51 60 69 78 87 96 105

58.207

7 16 25 34 43 52 61 70 79 88 97 106

7 16 25 34 43 52 61 70 79 88 97 106

plains why low values of taxable income was also a distinc-2.00008 17 26 35 44 53 62 71 80 89 98 107

8 17 26 35 44 53 62 71 80 89 98 107

8.2181

28.634

1.0000 tive feature, as seen in Figure 7.

0.57 - MORTALITYRATEUNDER5

00 . 0 9

18 27 36 45 54 63 72 81 90 99

253.50

0.54 - DAILYNEWSPAPERS
0 9 18 27 36 45 54 63 72 81 90

99

426.55

6. Conclusions and Future Work1 10 19 28 37 46 55 64 73 82 91 100
-1.0000 2 11 20 29 38 47 56 65 74 83 92 101

1 10 19 28 37 46 55 64 73 82 91 100 2 11 20 29 38 47 56 65 74 83 92 101

3 12 21 30 39 48 57 66 75 84 93 102
-2.000
4 13 22 31 40 49 58 67 76 85 94 103

172.68

3 12 21 30 39 48 57 66 75 84 93 102 4 13 22 31 40 49 58 67 76 85 94 103

285.45

We have introduced a relative density SOM (ReDSOM)-3.0005 14 23 32 41 50 59 68 77 86 95 104

5 14 23 32 41 50 59 68 77 86 95 104

91.864

144.34

6 15 24 33 42 51 60 69 78 87 96 105

6 15 24 33 42 51 60 69 78 87 96 105

visualization that is able to show various changes in clus-7 16 25 34 43 52 61 70 79 88 97 106

7 16 25 34 43 52 61 70 79 88 97 106

ter structures, such as emerging clusters, disappearing clus-12.1278 17 26 35 44 53 62 71 80 89 98 107

11.044

8 17 26 35 44 53 62 71 80 89 98 107

3.2461

7.0881 ters, cluster enlargements, cluster contractions, movement

4.1426 of cluster centroids, and changes in cluster density. ReD-

2.4212 SOM has been tested with real-life datasets, including large

1.4150 datasets from the Australian Taxation Office.

0.8270 Experiments using real-life datasets have shown that

0.4833 ReDSOM is capable of indicating actual changes, such as

the change in economic fortunes of South American coun-

Figure 6. Relative density visualization rd2 of the 2007 to the 2006 ATO dataset (top) and distance matrix visualization (bottom) of the 2007 ATO dataset.

tries between the 1980s and 1990s, or the change in tax policy for low income earners.
These structural changes can be analyzed further by looking into the migration patterns of the entities. Future work incorporating migration analysis is underway.

Further analysis of the discriminating characteristics of this new region lead to insights that are important to the taxation analysts. The top distinctive feature was found to relate to low income rebate amounts, as shown in Figure 7. It was noted, in comparing the values to the 2006 map, that the maximum value of the low income rebate amount had doubled. Without any other knowledge, a change in behavior was identified through the deployment of ReDSOM.
An investigation, conducted after discovering this behavioral change, found that it was caused by a change in government policy. In 2006, the Australian Government increased the Low Income Tax Offset from $235 to $600 per

Acknowledgement
This research has been supported by the Australian Taxation Office. The authors express their gratitude to Elea Gudgeon for providing data and domain expertise, to AusAID for providing a PhD scholarship to the first author, and to the reviewers for providing useful feedbacks. Map colors are based on www.ColorBrewer.org.
References
[1] G. Adomavicius and J. Bockstedt. C-trend: Temporal cluster graphs for identifying and visualizing trends in multi-

0.92 - LOW_INCM_RBT_AMT 0.44 - TXBL_INCM_AMT

0.47 - NET_TAX_AMT
593.88 396.10 198.32 0.5452
0.37 - TOTL_INCM_LOSS_AMT
163.0K 109.7K 56.35K 2.983K

0.44 - GROSS_TAX_AMT
55.54K

55.56K

37.05K

37.07K

18.56K

18.58K

72.576

91.886

0.32 - TOTL_TAX_WITHHELD_AMT
175.6K

49.98K

118.0K

33.43K

60.49K

16.87K

2.917K

327.33

Figure 7. Top distinctive features for the selected region in Figure 6.

attribute transactional data. IEEE TKDE, 20(6):721­735, 2008. [2] C. Aggarwal, J. Han, J. Wang, and P. Yu. A framework for clustering evolving data streams. VLDB, 29:81­92, 2003. [3] D. Chakrabarti, R. Kumar, and A. Tomkins. Evolutionary clustering. In ACM SIGKDD 2006, pages 554­560, New York, NY, USA, 2006. [4] G. Deboeck and T. Kohonen. Visual Explorations in Finance with Self-Organizing Maps. Springer-Verlag, London, 1998. [5] Denny and D. M. Squire. Visualization of cluster changes by comparing Self-Organizing Maps. In PAKDD 2005, volume 3518 of LNCS, pages 410­419. Springer, 2005. [6] Denny, G. J. Williams, and P. Christen. Exploratory hot spot profile analysis using interactive visual drill-down selforganizing maps. In PAKDD 2008, volume 5012 of LNCS, pages 536­543. Springer, 2008. [7] V. Ganti, J. Gehrke, R. Ramakrishnan, and W.-Y. Loh. A framework for measuring differences in data characteristics. Journal of Computer and System Sciences, 64:542­578, May 2002. [8] J. Han and M. Kamber. Data Mining: Concepts and Techniques (second edition). Morgan Kaufmann, San Francisco, CA, 2006. [9] S. Hido, T. Ide´, H. Kashima, H. Kubo, and H. Matsuzawa. Unsupervised change analysis using supervised learning. In PAKDD 2008, volume 5012 of LNCS, pages 148­159. Springer, 2008. [10] J. Iivarinen, T. Kohonen, J. Kangas, and S. Kaski. Visualizing the clusters on the Self-Organizing Map. In Conference on AI Research in Finland, volume 12, pages 122­126. Finnish AI Society, 1994. [11] S. Kaski and K. Lagus. Comparing Self-Organizing Maps. In ICANN'96, Bochum, Germany, volume 1112 of LNCS, pages 809­814. Springer, Berlin, 1996. [12] D. A. Keim. Information visualization and visual data mining. IEEE Trans. Vis. Comput. Graph., 8(1):1­8, 2002. [13] E. Keogh, J. Lin, and W. Truppel. Clustering of time series subsequences is meaningless: Implications for previous and

future research. In IEEE ICDM 2003, page 115, Washington, DC, USA, 2003. [14] T. Kohonen. Self-organized formation of topologically correct feature maps. Biological Cybernetics, 43:59­69, 1982. [15] T. Kohonen. Self-Organizing Maps (Third Edition), volume 30 of Springer Series in Information Sciences. Springer, Berlin, Heidelberg, 2001. [16] P. Lingras, M. Hogo, M. Snorek, and C. West. Temporal analysis of clusters of supermarket customers: conventional vs. interval set approach. Inf. Sci., 172(1-2):215­240, 2005. [17] H. Ritter, T. Martinetz, and K. Schulten. Neural Computation and Self-Organizing Maps; An Introduction. AddisonWesley Longman Publishing, Boston, USA, 1992. [18] J. F. Roddick and M. Spiliopoulou. A survey of temporal knowledge discovery paradigms and methods. IEEE TKDE, 14(4):750­767, 2002. [19] J. C. Schlimmer and R. H. Granger. Incremental learning from noisy data. Machine Learning, 1(3):317­354, 1986. [20] The Treasury - Australian Government. Press release no. 066, July 2006. http://www.treasurer.gov.au/. [21] V. Tryba, S. Metzen, and K. Goser. Designing basic integrated circuits by Self-Organizing Feature Maps. In NeuroN^imes'89. Intl. Workshop on Neural Networks and their Applications, pages 225­235, Nanterre, France, November 1989. ARC; SEE, EC2. [22] J. Vesanto and E. Alhoniemi. Clustering of the SelfOrganizing Map. IEEE TNN, 11(3):586­600, May 2000. [23] G. Widmer and M. Kubat. Learning in the presence of concept drift and hidden contexts. Machine Learning, 23(1):69­ 101, 1996. [24] World Bank. World Development Indicators 2003. The World Bank, Washington DC, 2003. [25] K. Yamanishi, J.-I. Takeuchi, G. Williams, and P. Milne. Online unsupervised outlier detection using finite mixtures with discounting learning algorithms. Data Mining and Knowledge Discovery, 8(3):275­300, 2004. [26] R. Zagha and G. T. Nankani, editors. Economic Growth in the 1990s: Learning from a Decade of Reform. World Bank Publications, Washington, DC, 2005.

